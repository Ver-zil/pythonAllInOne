{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c19e14d",
   "metadata": {},
   "source": [
    "# **Trial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfc615ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:37:02.399222Z",
     "start_time": "2024-05-03T06:37:01.130549Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora, models\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34d215f-8c76-4dfd-a077-0b03284c4ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22年入市，昨天早上几乎最高位离场，达到预期收益就收手，纪律性是对抗贪婪本性的唯一办法[OK]',\n",
       " '今天已经暴跌了，但是我认为下午会有国家队。。。这才第二天就砸盘这么猛，主力有点不给面子了，盲猜收盘之前会翻红，但是也让散户看明白了，什么政策什么经济复苏，还是那套玩意，就是嘎散户的钱，盲目入坑的绝对冷静了[doge_金箍]反正我是没入，不亏就是赚了',\n",
       " '昨天在店内一边吃鸡翅一边用手机看股票。一个乞丐进来乞讨，我给他一块鸡翅后继续看股票。乞丐啃着鸡翅没走，也在一旁看着，他说：“长期均线金叉，KDJ数值底部反复钝化，MACD底背离，能量潮喇叭口扩大，这股要涨了。”我惊诧地问：“这个你也懂？” 乞丐说：“不懂我能有今天？”',\n",
       " '几天的大阳线把股民们以前的记忆都干没了[doge]，你们根本不知道有多疯狂，我一个从未接触过股票的朋友都开户来问我怎么玩了，你可知道他竟然连板块都不会找股票代码什么是沪深的也不知道就投了几万块进去',\n",
       " '经历2008年股灾的老人回忆到，股市有政策底，还有市场底。政策到位，上涨一轮，引人下场后再跌停，让你来不及逃生。我巨亏40%，狠狠心抛了，逃过了后面的下跌。',\n",
       " '一个特别微观又直观的例子：大家可以看下各银行的大额存单转让区，这里基本都是居民存款，节前转让收益率上升了10-20bp。说明这轮行情真真实实的撬动了银行存款，之前那么多zc没办到的事情这次竟然办到了。',\n",
       " '我始终坚信，一个赌场般的市场，绝对不会让大多数人赚到钱。',\n",
       " '今天已经开始技术性回调了[脱单doge]',\n",
       " '别人都是看涨，导致目前赛道过于拥挤。而老王直接看空，以此来安慰没上车的群体，别出心裁、另辟蹊径。看似在谈经济，实则在玩自媒体。老王高啊[支持]',\n",
       " '无论是涨是跌，最终能赚到并且保住这些钱的人都是极少数人，关键在于能不能清晰地认识到在这些人里面包不包括自己。',\n",
       " '我重申一下：股市是穷人最少的地方，中产最多的地方。\\n在股市放水，造成的结果就是，中产变成富人，穷人还是穷人。\\n【Doge】动动猪脑子想想，穷人能有多少钱玩？万儿八千，好点的十万八万，顶天了。\\n富人可是百万千万。\\n股市翻一倍，富人100万变200万，穷人5万变10万。\\n\\n 贫富差距增加了 = 最终贫富差距 - 初始贫富差距 = 190万 - 95万 = 95万\\n\\n因此。\\n贫富差距增加了95万！！这表明，尽管穷人的财富也增加了，但富人财富的增加比例更大，导致贫富差距扩大，你告诉我这是在放水？',\n",
       " '记住，中国玩股票的人，不到10个点。\\n所以中国股市从来就不能反应中国经济。\\nA股从来就是一个赌字。\\n别指望分析有什么用',\n",
       " '你涨，我也不会买，你跌，我只会笑哈哈',\n",
       " '[喜极而泣]曾经做空能赚钱，我连开户资格都没有',\n",
       " '当我这种完全不关注、圈子和股票完全没交集的人也刷到了股市的视频....[微笑]',\n",
       " '最后几句话精辟，如果拜佛有用，你庙门都进不去。如果种地能致富，那农民将无地可种。如果股票能赚钱，你连证劵账户都开不了。社会的基本规律[doge]',\n",
       " '不懂，不买，祝福。',\n",
       " '别参照519行情，一个最大的区别就是那时就算在高位接盘了后来也是真的能解套，现在要是高位接盘那就套一辈子',\n",
       " '别人恐惧我贪婪，满仓！[doge]',\n",
       " '为什么大多数散户在牛市中你也赚不了钱？因为你是人，你克服不了人性的贪婪和恐惧，你总想着你能逃顶，事实上给你多少次机会，你都逃不了。\\n举个例子：你在3000点买入，接下来大盘连续疯涨到3600点，你浮盈大概20%，这时候来几根大阴线调整，你连忙卖出，让你损失了5%的利润。调整几天后，继续涨到3700点，因为贪婪，你又满仓进来，接下来一路涨到4000点。开始新一轮调整，有了上次的教训，你不再轻易抛出筹码，果然，调整到3800点后继续上涨到4200点，这时候你把手里的仅有的子弹也打了进去，甚至开始借钱融资。接下来大盘上涨到4300点后开始开始调整，你还是维持你上次的判断，只是良性调整，结果开始大幅下杀，你因为一直加仓，有些个股甚至开始亏损，仅仅一天让你的利润没了一半，你心态崩了，你不认输，因为你觉得卖出去了就彻底没希望了，然后第二天开盘继续杀，你觉得已经大跌两天了，等明天反弹了你就逃，结果第三天继续跌，你的利润全没了。\\n\\n所有人都觉得别人贪婪时我恐惧，别人恐惧时我贪婪，事实上别人贪婪时你更贪婪，别人恐惧时不更恐惧。\\n\\n在股市里改变一个人，只需要三根大阴线。']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\stock\\BV1LuxZeVE25.json'\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as file:\n",
    "    # 加载 JSON 数据\n",
    "    data = json.load(file)\n",
    "    \n",
    "doc_data = [info['review'] for info in data]\n",
    "doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd85988-a1a6-4c28-99ef-df85d9c577b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\11435\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.483 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 读取停用词，并去停用词\n",
    "stopwords_path1 = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\corpus\\stopwords_scu.txt'\n",
    "with open(stopwords_path1, 'r', encoding='utf-8') as f:\n",
    "    stopwords1 = set([line.strip() for line in f])\n",
    "\n",
    "stopwords_path2 = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\corpus\\stopwords_hit.txt'\n",
    "with open(stopwords_path2, 'r', encoding='utf-8') as f:\n",
    "    stopwords2 = set([line.strip() for line in f])\n",
    "\n",
    "stopwords = stopwords1.union(stopwords2)\n",
    "\n",
    "texts = []\n",
    "for doc in doc_data:\n",
    "    words = jieba.cut(doc)\n",
    "    filter_words = [word for word in words if word not in stopwords and word.strip() != '']\n",
    "    texts.append(filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae832eb-7c6c-48e3-9edd-803cffd5066b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 过滤频次\n",
    "FREQ_LIMIT = 1\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > FREQ_LIMIT]for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bad7db8-5739-4005-83a8-7d44ca6dd8fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建词典和语料库\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb663406-0b36-471f-b1fa-f1618d4de334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3387b1ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:37:19.467595Z",
     "start_time": "2024-05-03T06:37:19.445593Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提取主题向量\n",
    "lda_features = []\n",
    "for document in corpus:\n",
    "    topic_distribution = lda.get_document_topics(document)\n",
    "    lda_features.append([topic[1] for topic in topic_distribution])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99fb7c13-92c5-4cf0-858d-85f53cd163c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd83399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:38:02.928187Z",
     "start_time": "2024-05-03T06:38:02.912987Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(2, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af06009-73e3-4951-a818-db001f673d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42ab622c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:40:43.022516Z",
     "start_time": "2024-05-03T06:40:43.009517Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "D:\\tool\\miniconda3\\envs\\allInOne\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertModel were not initialized from the model checkpoint at D:\\tool\\toolkit\\nlp\\distiluse-base-multilingual-cased-v2-finetuned-stsb_multi_mt-es and are newly initialized: ['embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 加载已经训练好的bert模型\n",
    "model_path = r'D:\\tool\\toolkit\\nlp\\distiluse-base-multilingual-cased-v2-finetuned-stsb_multi_mt-es'\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07b56c95-9b2a-43bd-8848-18b29e0bb28f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_features = []\n",
    "for article in doc_data:\n",
    "    # 对文章进行分词和编码\n",
    "    encoded_input = tokenizer(article, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    # 获取模型的输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    # 获取CLS token的嵌入作为文章的特征向量\n",
    "    bert_features.append(outputs.last_hidden_state[:, 0, :].squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "962648a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:39:13.503336Z",
     "start_time": "2024-05-03T06:39:13.488177Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lda和bert特征拼接\n",
    "concatenated_features = []\n",
    "for bert_feature, lda_feature in zip(bert_features, lda_features):\n",
    "    # 将BERT特征向量和LDA主题向量拼接\n",
    "    concatenated_feature = np.concatenate((bert_feature, lda_feature))\n",
    "    concatenated_features.append(concatenated_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12ad01ab-7567-45c7-b4a7-d2f21b51bafd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义自编码器\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder  = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.en_fc = nn.Linear(64, hidden_size)\n",
    "        self.de_fc = nn.Linear(hidden_size, 64)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        en = self.encoder(x)\n",
    "        code = self.en_fc(en)\n",
    "        de = self.de_fc(code)\n",
    "        decoded = self.decoder(de)\n",
    "        return code, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef2eb34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:02:58.651656Z",
     "start_time": "2024-05-03T07:02:58.429432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf3a300-cf9d-4712-a8d5-7a5c68627298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de8034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:04:32.191987Z",
     "start_time": "2024-05-03T07:04:32.176893Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
