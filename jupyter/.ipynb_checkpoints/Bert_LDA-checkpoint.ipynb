{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec08b3-7b43-4054-adf9-127fc81d71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora, models\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9dff64-33dc-431d-b88f-0ecaf100681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\stock\\BV1LuxZeVE25.json'\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as file:\n",
    "    # 加载 JSON 数据\n",
    "    data = json.load(file)\n",
    "    \n",
    "doc_data = [info['review'] for info in data]\n",
    "doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bf3fd1-c8d2-4b4b-b22e-f94f93b7077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取停用词，并去停用词\n",
    "stopwords_path1 = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\corpus\\stopwords_scu.txt'\n",
    "with open(stopwords_path1, 'r', encoding='utf-8') as f:\n",
    "    stopwords1 = set([line.strip() for line in f])\n",
    "\n",
    "stopwords_path2 = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\corpus\\stopwords_hit.txt'\n",
    "with open(stopwords_path2, 'r', encoding='utf-8') as f:\n",
    "    stopwords2 = set([line.strip() for line in f])\n",
    "\n",
    "stopwords = stopwords1.union(stopwords2)\n",
    "\n",
    "texts = []\n",
    "for doc in doc_data:\n",
    "    words = jieba.cut(doc)\n",
    "    filter_words = [word for word in words if word not in stopwords and word.strip() != '']\n",
    "    texts.append(filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c802a-25de-4431-b18d-3a7beff6658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤频次\n",
    "FREQ_LIMIT = 1\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > FREQ_LIMIT]for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85c00f-b95c-4eab-b34e-65cba7f29aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词典和语料库\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ff167-6252-4f8c-acd5-aefc74f1d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练多个LDA模型，并计算每个模型的困惑度\n",
    "models_perplexity = []\n",
    "num_topics_range = range(2, 21)  # 从2到20个主题\n",
    "\n",
    "for num_topics in num_topics_range:\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    models_perplexity.append((num_topics, perplexity))\n",
    "\n",
    "# 打印每个模型的主题数和困惑度\n",
    "for num_topics, perplexity in models_perplexity:\n",
    "    print(f\"Number of Topics: {num_topics}, Perplexity: {perplexity}\")\n",
    "\n",
    "# 选择困惑度最低的模型\n",
    "best_num_topics = min(models_perplexity, key=lambda x: x[1])[0]\n",
    "print(f\"Best number of topics: {best_num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fbb13-e864-4b68-907e-59eecbd5c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(models_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39f549-9ab6-4f0d-a63d-bc1ea293ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topic = 5\n",
    "lda = models.LdaModel(corpus, num_topics=num_topic, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfad8eb-7bdb-4a97-8069-14ba3de23ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# 计算主题一致性\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score: ', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325ccd6-11dd-48e0-b729-387f1850c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='u_mass').get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52e859-b8e1-4c22-be89-04efd659e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_data_fill(topic_distribution, topic_num):\n",
    "    features = []\n",
    "    if len(topic_distribution) < topic_num:\n",
    "        features = np.zeros(topic_num).tolist()\n",
    "        for topic in topic_distribution:\n",
    "            features[topic[0]] = topic[1]\n",
    "    else:\n",
    "        features = [topic[1] for topic in topic_distribution]\n",
    "    return features\n",
    "\n",
    "# 提取主题向量\n",
    "lda_features = []\n",
    "for document in corpus:\n",
    "    topic_distribution = lda.get_document_topics(document)\n",
    "    lda_features.append(topic_data_fill(topic_distribution, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324495e-2471-4e29-99c9-74a203746edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b5a86-0609-48a5-be83-0d184228677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检测程序\n",
    "for idx, features in enumerate(lda_features):\n",
    "    if len(features) < 5:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ac955-895a-4dd5-8311-36ae02096cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85742c99-f4d5-407e-b5f9-76b7a9542059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6cc4d-1682-469a-8826-48fe3b72b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载已经训练好的bert模型\n",
    "model_path = r'D:\\tool\\toolkit\\nlp\\distiluse-base-multilingual-cased-v2-finetuned-stsb_multi_mt-es'\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98373d8-786b-4b88-b24c-f106ee04fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_features = []\n",
    "for article in doc_data:\n",
    "    # 对文章进行分词和编码\n",
    "    encoded_input = tokenizer(article, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    # 获取模型的输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    # 获取CLS token的嵌入作为文章的特征向量\n",
    "    bert_features.append(outputs.last_hidden_state[:, 0, :].squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e9261-28eb-43a7-ad02-6f2011101848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda和bert特征拼接\n",
    "concatenated_features = []\n",
    "for bert_feature, lda_feature in zip(bert_features, lda_features):\n",
    "    # 将BERT特征向量和LDA主题向量拼接\n",
    "    concatenated_feature = np.concatenate((bert_feature, lda_feature))\n",
    "    concatenated_features.append(concatenated_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3dab5-b682-462d-9150-81bc6821af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义自编码器\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder  = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.en_fc = nn.Linear(64, hidden_size)\n",
    "        self.de_fc = nn.Linear(hidden_size, 64)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        en = self.encoder(x)\n",
    "        code = self.en_fc(en)\n",
    "        de = self.de_fc(code)\n",
    "        decoded = self.decoder(de)\n",
    "        return code, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb4773-8a1d-45c4-ab04-62b0f57cb33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "# 模型默认参数为float32，如果想要用double(float64)来训练的话，model=model.double()\n",
    "tensor_data = torch.tensor(concatenated_features, dtype=torch.float32)\n",
    "dataset = TensorDataset(tensor_data)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 开始训练\n",
    "HIDDEN_SIZE = 32\n",
    "input_dim = len(concatenated_features[0])\n",
    "\n",
    "model = AutoEncoder(input_dim, HIDDEN_SIZE)\n",
    "criterion = nn.MSELoss()  # 均方误差损失\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "model = model.to(\"cpu\")\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data,) in enumerate(data_loader):\n",
    "        # 正向传播\n",
    "        outputs = model(data)[1]\n",
    "        loss = criterion(outputs, data)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c3b35-4110-490b-b7b1-67735376e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取文本encoder后的结果\n",
    "encoder_features = model(dataset)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209dc8ea-3941-4672-8b97-032d8480dce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 用KM算法进行聚类，并提取关键词\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "K = 5\n",
    "\n",
    "# 创建KMeans实例，指定聚类数目K\n",
    "kmeans = KMeans(n_clusters=K)\n",
    "\n",
    "# 拟合模型\n",
    "kmeans.fit(encoder_features)\n",
    "\n",
    "# 预测每个数据点的聚类标签\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# 获取聚类中心\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# 打印结果\n",
    "print(\"Cluster labels:\", labels)\n",
    "print(\"Cluster centers:\", centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc92f2d-df83-4ad5-af5f-d1e999433d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据聚类的结果对主题词进行提炼\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 为每个类别收集文档\n",
    "category_docs = {}\n",
    "for doc, label in zip(texts, labels):\n",
    "    if label not in category_docs:\n",
    "        category_docs[label] = []\n",
    "    category_docs[label].append(doc)\n",
    "\n",
    "# 初始化TF-IDF向量化器\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 将所有文档转换为TF-IDF向量\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 初始化类别主题词字典\n",
    "category_topics = {}\n",
    "\n",
    "# 计算每个类别的文档数量\n",
    "category_doc_counts = {label: len(docs) for label, docs in category_docs.items()}\n",
    "\n",
    "# 计算每个类别的TF-IDF平均向量\n",
    "for label, docs in category_docs.items():\n",
    "    # 选择当前类别的所有文档的TF-IDF向量\n",
    "    category_tfidf = X[labels == label]\n",
    "    \n",
    "    # 计算平均TF-IDF向量\n",
    "    category_avg_tfidf = category_tfidf.mean(axis=0)\n",
    "    \n",
    "    # 归一化平均TF-IDF向量\n",
    "    category_avg_tfidf = normalize(category_avg_tfidf, norm='l1')\n",
    "    \n",
    "    # 存储类别的平均TF-IDF向量\n",
    "    category_topics[label] = category_avg_tfidf\n",
    "    \n",
    "# 获取特征名称（词汇）\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 为每个类别提取主题词\n",
    "def get_category_topic(label, num_words=10):\n",
    "    # 获取当前类别的平均TF-IDF向量\n",
    "    avg_tfidf = category_topics[label]\n",
    "    \n",
    "    # 获取词汇表中每个词的索引\n",
    "    word_indices = avg_tfidf.indices\n",
    "    \n",
    "    # 获取每个词的TF-IDF值\n",
    "    word_scores = avg_tfidf.data\n",
    "    \n",
    "    # 按TF-IDF值降序排列词\n",
    "    sorted_indices = word_indices[np.argsort(-word_scores)]\n",
    "    \n",
    "    # 返回TF-IDF值最高的N个词作为主题词\n",
    "    top_n_words = [feature_names[index] for index in sorted_indices[:num_words]]\n",
    "    return top_n_words\n",
    "\n",
    "# 为每个类别提取主题词\n",
    "category_topic_words = {label: get_category_topic(label) for label in category_topics.keys()}\n",
    "\n",
    "for label, topic_words in category_topic_words.items():\n",
    "    print(f\"Category {label} Topic Words: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de3f19-1e20-4bc3-8aca-a966874fd913",
   "metadata": {},
   "source": [
    "# 计算评价指标主题一致性、sc系数、jc系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77910f-4d3a-4963-bbf2-fd37af89926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指标umass计算方式,如果采用UCI计算方式,把window_size的参数进行调整\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "\n",
    "def build_co_occurrence_matrix(documents, window_size=2):\n",
    "    \"\"\"构建词共现矩阵\"\"\"\n",
    "    co_occurrence = defaultdict(int)\n",
    "    for doc in documents:\n",
    "        words = list(doc)\n",
    "        for i in range(len(words) - window_size + 1):\n",
    "            for pair in combinations(words[i:i + window_size], 2):\n",
    "                co_occurrence[pair] += 1\n",
    "    return co_occurrence\n",
    "\n",
    "def calculate_pmi(co_occurrence):\n",
    "    \"\"\"计算点互信息（PMI）\"\"\"\n",
    "    # 计算每个单独词的总出现次数\n",
    "    word_counts = Counter()\n",
    "    for pair, count in co_occurrence.items():\n",
    "        word_counts[pair[0]] += count\n",
    "        word_counts[pair[1]] += count\n",
    "\n",
    "    total_documents = sum(co_occurrence.values())\n",
    "\n",
    "    for pair, count in co_occurrence.items():\n",
    "        pair_prob = count * 2 / total_documents\n",
    "        word1, word2 = pair\n",
    "        word1_prob = word_counts[word1] / total_documents\n",
    "        word2_prob = word_counts[word2] / total_documents\n",
    "        pmi = np.log(pair_prob / (word1_prob * word2_prob)) if (word1_prob * word2_prob) > 0 else 0\n",
    "        co_occurrence[pair] = pmi\n",
    "\n",
    "    return co_occurrence\n",
    "\n",
    "def umass_score(texts, top_n=-1):\n",
    "    co_occurrence = build_co_occurrence_matrix(texts)\n",
    "    pmi_matrix = calculate_pmi(co_occurrence)\n",
    "    # 筛选出存在于PMI矩阵中的词对\n",
    "    # valid_pairs = [pair for pair in pmi_matrix.keys() if pmi_matrix[pair] > 0]\n",
    "    # top_pairs = sorted(valid_pairs, key=lambda x: pmi_matrix[x], reverse=True)[:top_n]\n",
    "    # umass = sum(pmi_matrix[pair] for pair in top_pairs) / len(top_pairs)\n",
    "    umass = sum(pmi_matrix.values())/len(pmi_matrix)\n",
    "    return umass\n",
    "\n",
    "\n",
    "coherence_scores = {label: umass_score(cluster) for label, cluster in category_docs.items()}\n",
    "average_coherence = sum(coherence_scores.values()) / len(coherence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929fa591-cb40-44af-9898-9db173408096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc系数sklearn已有现成的包能进行计算->适合参考\n",
    "from sklearn.metrics import silhouette_score\n",
    "def sc(features, labels):\n",
    "    return silhouette_score(features, labels)\n",
    "\n",
    "silhouette_avg = sc()\n",
    "print(f\"轮廓系数: {silhouette_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92833814-64c5-4f08-ac64-c8dcb536340e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# jc系数/散度->无意义,对于硬分类,算出来的结果基本都是1\n",
    "def jaccard_distance(u, v):\n",
    "    intersection = len(set(u) & set(v))\n",
    "    union = len(set(u) | set(v))\n",
    "    return 1 - intersection / union if union != 0 else 0\n",
    "\n",
    "def jaccard_coefficient(centers):\n",
    "    n_clusters = len(centers)\n",
    "    jaccard_sum = 0\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(i + 1, n_clusters):\n",
    "            distance = jaccard_distance(centers[i], centers[j])\n",
    "            jaccard_sum += distance\n",
    "    \n",
    "    return jaccard_sum / (n_clusters * (n_clusters - 1) / 2)\n",
    "\n",
    "# 示例使用\n",
    "# centers = np.array([...])  # 聚类中心\n",
    "# jc = jaccard_coefficient(centers)\n",
    "# print(\"Jaccard 散度:\", jc)\n",
    "jaccard_coefficient(centers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (trainingField)",
   "language": "python",
   "name": "pycharm-d26ea172"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
