{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8425e06-2067-48eb-bcc5-8a421628175a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora, models\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4da129-6694-4533-a634-4ef3b554f5b5",
   "metadata": {},
   "source": [
    "# 数据预处理区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69907ebb-eb55-44dc-bc52-7dec5bb32808",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22年入市，昨天早上几乎最高位离场，达到预期收益就收手，纪律性是对抗贪婪本性的唯一办法[OK]',\n",
       " '今天已经暴跌了，但是我认为下午会有国家队。。。这才第二天就砸盘这么猛，主力有点不给面子了，盲猜收盘之前会翻红，但是也让散户看明白了，什么政策什么经济复苏，还是那套玩意，就是嘎散户的钱，盲目入坑的绝对冷静了[doge_金箍]反正我是没入，不亏就是赚了',\n",
       " '昨天在店内一边吃鸡翅一边用手机看股票。一个乞丐进来乞讨，我给他一块鸡翅后继续看股票。乞丐啃着鸡翅没走，也在一旁看着，他说：“长期均线金叉，KDJ数值底部反复钝化，MACD底背离，能量潮喇叭口扩大，这股要涨了。”我惊诧地问：“这个你也懂？” 乞丐说：“不懂我能有今天？”',\n",
       " '几天的大阳线把股民们以前的记忆都干没了[doge]，你们根本不知道有多疯狂，我一个从未接触过股票的朋友都开户来问我怎么玩了，你可知道他竟然连板块都不会找股票代码什么是沪深的也不知道就投了几万块进去',\n",
       " '经历2008年股灾的老人回忆到，股市有政策底，还有市场底。政策到位，上涨一轮，引人下场后再跌停，让你来不及逃生。我巨亏40%，狠狠心抛了，逃过了后面的下跌。',\n",
       " '一个特别微观又直观的例子：大家可以看下各银行的大额存单转让区，这里基本都是居民存款，节前转让收益率上升了10-20bp。说明这轮行情真真实实的撬动了银行存款，之前那么多zc没办到的事情这次竟然办到了。',\n",
       " '我始终坚信，一个赌场般的市场，绝对不会让大多数人赚到钱。',\n",
       " '今天已经开始技术性回调了[脱单doge]',\n",
       " '别人都是看涨，导致目前赛道过于拥挤。而老王直接看空，以此来安慰没上车的群体，别出心裁、另辟蹊径。看似在谈经济，实则在玩自媒体。老王高啊[支持]',\n",
       " '无论是涨是跌，最终能赚到并且保住这些钱的人都是极少数人，关键在于能不能清晰地认识到在这些人里面包不包括自己。',\n",
       " '我重申一下：股市是穷人最少的地方，中产最多的地方。\\n在股市放水，造成的结果就是，中产变成富人，穷人还是穷人。\\n【Doge】动动猪脑子想想，穷人能有多少钱玩？万儿八千，好点的十万八万，顶天了。\\n富人可是百万千万。\\n股市翻一倍，富人100万变200万，穷人5万变10万。\\n\\n 贫富差距增加了 = 最终贫富差距 - 初始贫富差距 = 190万 - 95万 = 95万\\n\\n因此。\\n贫富差距增加了95万！！这表明，尽管穷人的财富也增加了，但富人财富的增加比例更大，导致贫富差距扩大，你告诉我这是在放水？',\n",
       " '记住，中国玩股票的人，不到10个点。\\n所以中国股市从来就不能反应中国经济。\\nA股从来就是一个赌字。\\n别指望分析有什么用',\n",
       " '你涨，我也不会买，你跌，我只会笑哈哈',\n",
       " '[喜极而泣]曾经做空能赚钱，我连开户资格都没有',\n",
       " '当我这种完全不关注、圈子和股票完全没交集的人也刷到了股市的视频....[微笑]',\n",
       " '最后几句话精辟，如果拜佛有用，你庙门都进不去。如果种地能致富，那农民将无地可种。如果股票能赚钱，你连证劵账户都开不了。社会的基本规律[doge]',\n",
       " '不懂，不买，祝福。',\n",
       " '别参照519行情，一个最大的区别就是那时就算在高位接盘了后来也是真的能解套，现在要是高位接盘那就套一辈子',\n",
       " '别人恐惧我贪婪，满仓！[doge]',\n",
       " '为什么大多数散户在牛市中你也赚不了钱？因为你是人，你克服不了人性的贪婪和恐惧，你总想着你能逃顶，事实上给你多少次机会，你都逃不了。\\n举个例子：你在3000点买入，接下来大盘连续疯涨到3600点，你浮盈大概20%，这时候来几根大阴线调整，你连忙卖出，让你损失了5%的利润。调整几天后，继续涨到3700点，因为贪婪，你又满仓进来，接下来一路涨到4000点。开始新一轮调整，有了上次的教训，你不再轻易抛出筹码，果然，调整到3800点后继续上涨到4200点，这时候你把手里的仅有的子弹也打了进去，甚至开始借钱融资。接下来大盘上涨到4300点后开始开始调整，你还是维持你上次的判断，只是良性调整，结果开始大幅下杀，你因为一直加仓，有些个股甚至开始亏损，仅仅一天让你的利润没了一半，你心态崩了，你不认输，因为你觉得卖出去了就彻底没希望了，然后第二天开盘继续杀，你觉得已经大跌两天了，等明天反弹了你就逃，结果第三天继续跌，你的利润全没了。\\n\\n所有人都觉得别人贪婪时我恐惧，别人恐惧时我贪婪，事实上别人贪婪时你更贪婪，别人恐惧时不更恐惧。\\n\\n在股市里改变一个人，只需要三根大阴线。']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\stock\\BV1LuxZeVE25.json'\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as file:\n",
    "    # 加载 JSON 数据\n",
    "    data = json.load(file)\n",
    "    \n",
    "doc_data = [info['review'] for info in data]\n",
    "doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2ae9cc-6234-4f34-ac32-466b35e3affb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\11435\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.607 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 读取停用词，并去停用词\n",
    "stopwords_path1 = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\corpus\\stopwords_scu.txt'\n",
    "with open(stopwords_path1, 'r', encoding='utf-8') as f:\n",
    "    stopwords1 = set([line.strip() for line in f])\n",
    "\n",
    "stopwords_path2 = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\corpus\\stopwords_hit.txt'\n",
    "with open(stopwords_path2, 'r', encoding='utf-8') as f:\n",
    "    stopwords2 = set([line.strip() for line in f])\n",
    "\n",
    "stopwords = stopwords1.union(stopwords2)\n",
    "\n",
    "texts = []\n",
    "for doc in doc_data:\n",
    "    words = jieba.cut(doc)\n",
    "    filter_words = [word for word in words if word not in stopwords and word.strip() != '']\n",
    "    texts.append(filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68f79401-7643-46c6-a6da-3bf6eae9c775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 过滤频次，长文本采取这种形式没问题，但是短文本本身信息就少，如果再采取这种形式，那就有很大问题了\n",
    "FREQ_LIMIT = 0\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > FREQ_LIMIT]for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6e0d4-40d4-41b5-b720-fb8cdb6cd944",
   "metadata": {},
   "source": [
    "# 模型区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d67f3ba6-72dd-4ccd-b4c1-1911c6e26629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建词典和语料库\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1093af01-d652-44fb-a896-24ecaaaac721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "717ae110-eb6c-4e8c-b852-a709cc3d7031",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.013*\"年\" + 0.013*\"政策\" + 0.013*\"散户\" + 0.007*\"doge\" + 0.007*\"股市\" + 0.007*\"不\" + 0.007*\"钱\" + 0.007*\"市场\" + 0.007*\"上涨\" + 0.007*\"赚钱\"')\n",
      "(1, '0.027*\"都\" + 0.018*\"股票\" + 0.014*\"不\" + 0.014*\"没\" + 0.014*\"乞丐\" + 0.014*\"鸡翅\" + 0.014*\"知道\" + 0.010*\"doge\" + 0.010*\"懂\" + 0.010*\"接盘\"')\n",
      "(2, '0.030*\"贪婪\" + 0.026*\"恐惧\" + 0.026*\"调整\" + 0.022*\"点\" + 0.018*\"继续\" + 0.018*\"时\" + 0.013*\"不\" + 0.013*\"觉得\" + 0.013*\"利润\" + 0.009*\"都\"')\n",
      "(3, '0.017*\"都\" + 0.017*\"老王\" + 0.010*\"涨\" + 0.010*\"doge\" + 0.010*\"经济\" + 0.010*\"赚\" + 0.010*\"不\" + 0.010*\"跌\" + 0.010*\"人\" + 0.010*\"玩\"')\n",
      "(4, '0.032*\"万\" + 0.032*\"穷人\" + 0.027*\"股市\" + 0.027*\"贫富差距\" + 0.022*\"增加\" + 0.022*\"富人\" + 0.017*\"人\" + 0.017*\"中国\" + 0.017*\"95\" + 0.011*\"钱\"')\n"
     ]
    }
   ],
   "source": [
    "# 打印主题\n",
    "topics = lda.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "639cf2d7-0ec4-4692-b15f-7f61ce793406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档-主题特征:\n",
      "文档 0 的主题分布: [(0, 0.95778275), (1, 0.010527902), (2, 0.010632243), (3, 0.010528978), (4, 0.010528151)]\n",
      "文档 1 的主题分布: [(0, 0.9799014), (1, 0.005026091), (2, 0.0050231027), (3, 0.0050331783), (4, 0.0050162445)]\n",
      "文档 2 的主题分布: [(0, 0.0038578233), (1, 0.9845681), (2, 0.0038653682), (3, 0.0038561719), (4, 0.0038525146)]\n",
      "文档 3 的主题分布: [(0, 0.0062679085), (1, 0.9748992), (2, 0.0062766434), (3, 0.00628278), (4, 0.006273494)]\n",
      "文档 4 的主题分布: [(0, 0.9722969), (1, 0.0069069094), (2, 0.006927575), (3, 0.0069067725), (4, 0.0069618383)]\n",
      "文档 5 的主题分布: [(0, 0.006069367), (1, 0.9757157), (2, 0.0060706446), (3, 0.0060675335), (4, 0.006076797)]\n",
      "文档 6 的主题分布: [(0, 0.020191688), (1, 0.02000307), (2, 0.02015211), (3, 0.020163342), (4, 0.9194898)]\n",
      "文档 7 的主题分布: [(0, 0.028871493), (1, 0.028813083), (2, 0.02870924), (3, 0.88503087), (4, 0.028575327)]\n",
      "文档 8 的主题分布: [(0, 0.007701682), (1, 0.0077283634), (2, 0.007706832), (3, 0.9691422), (4, 0.0077209356)]\n",
      "文档 9 的主题分布: [(0, 0.011168878), (1, 0.011188025), (2, 0.011230008), (3, 0.9551968), (4, 0.01121628)]\n",
      "文档 10 的主题分布: [(0, 0.0026679547), (1, 0.0026683614), (2, 0.0026675968), (3, 0.0026713735), (4, 0.9893247)]\n",
      "文档 11 的主题分布: [(0, 0.011140381), (1, 0.011156162), (2, 0.01112317), (3, 0.011158442), (4, 0.9554218)]\n",
      "文档 12 的主题分布: [(0, 0.03334196), (1, 0.033531405), (2, 0.865909), (3, 0.03387661), (4, 0.033341065)]\n",
      "文档 13 的主题分布: [(0, 0.898712), (1, 0.025802923), (2, 0.025162285), (3, 0.02531825), (4, 0.02500459)]\n",
      "文档 14 的主题分布: [(0, 0.012537533), (1, 0.012696493), (2, 0.012624148), (3, 0.012579961), (4, 0.9495619)]\n",
      "文档 15 的主题分布: [(0, 0.008356692), (1, 0.96660554), (2, 0.008343988), (3, 0.008353578), (4, 0.008340188)]\n",
      "文档 16 的主题分布: [(0, 0.04011628), (1, 0.041094717), (2, 0.8385294), (3, 0.04015753), (4, 0.040102128)]\n",
      "文档 17 的主题分布: [(0, 0.013336223), (1, 0.9466552), (2, 0.013335491), (3, 0.013337134), (4, 0.01333592)]\n",
      "文档 18 的主题分布: [(0, 0.04045658), (1, 0.04072551), (2, 0.8382512), (3, 0.040562198), (4, 0.040004488)]\n",
      "文档 19 的主题分布: [(0, 0.0013464955), (1, 0.0013462614), (2, 0.9946143), (3, 0.0013460169), (4, 0.001346885)]\n"
     ]
    }
   ],
   "source": [
    "# 输出文档-主题特征\n",
    "# 提取主题向量\n",
    "lda_features = []\n",
    "print(\"文档-主题特征:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    topic_dist = lda.get_document_topics(doc, minimum_probability=0)\n",
    "    lda_features.append([topic[1] for topic in topic_dist])\n",
    "    print(f\"文档 {i} 的主题分布: {topic_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54bb60d8-f74c-41ab-b161-a9eb46e7957c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.010625031, 0.010528611, 0.95776933, 0.010530813, 0.010546163],\n",
       " [0.0050101015, 0.97993976, 0.0050173197, 0.005018049, 0.0050147315],\n",
       " [0.0038598964, 0.0038728954, 0.98455, 0.0038562494, 0.0038609495],\n",
       " [0.0062660957, 0.9749283, 0.006264286, 0.006255158, 0.00628615],\n",
       " [0.0069213216, 0.0069121905, 0.006922421, 0.0068990067, 0.97234505],\n",
       " [0.0060786325, 0.0061066668, 0.97566116, 0.0060629062, 0.0060906624],\n",
       " [0.02027632, 0.02050105, 0.9189749, 0.020008706, 0.020239016],\n",
       " [0.028610343, 0.02891074, 0.028629733, 0.8851864, 0.028662764],\n",
       " [0.0077132075, 0.0077310293, 0.007711219, 0.007694806, 0.9691497],\n",
       " [0.011168691, 0.955342, 0.01115433, 0.011190406, 0.01114462],\n",
       " [0.9893129, 0.0026725233, 0.0026704709, 0.002667641, 0.0026765112],\n",
       " [0.01115763, 0.01116617, 0.011148437, 0.011114023, 0.9554137],\n",
       " [0.03338787, 0.033509616, 0.033415284, 0.8663504, 0.03333686],\n",
       " [0.02502412, 0.8997781, 0.025017707, 0.025008114, 0.02517198],\n",
       " [0.012602746, 0.9497181, 0.012554433, 0.012503769, 0.012620925],\n",
       " [0.008341103, 0.008407312, 0.008347181, 0.008349568, 0.9665549],\n",
       " [0.04038768, 0.04214917, 0.040882707, 0.040015776, 0.83656466],\n",
       " [0.013334903, 0.013335666, 0.9466556, 0.01333801, 0.013335806],\n",
       " [0.64063734, 0.23757675, 0.04006856, 0.041035343, 0.04068203],\n",
       " [0.9946109, 0.0013521393, 0.0013462105, 0.0013444292, 0.0013462987]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9702a723-ebd5-4a4c-8039-a390aecd8cec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 5, 1: 5, 4: 5, 3: 2, 0: 3}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dict = {}\n",
    "for row in np.array(lda_features):\n",
    "    max_index = np.argmax(row)  # 获取最大值的索引\n",
    "    if max_index in count_dict:\n",
    "        count_dict[max_index] += 1  # 如果索引已存在，计数加1\n",
    "    else:\n",
    "        count_dict[max_index] = 1  # 如果索引不存在，初始化计数为1\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6369c76-8725-4bb8-b997-aa9679c7d8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\tool\\miniconda3\\envs\\allInOne\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "D:\\tool\\miniconda3\\envs\\allInOne\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels: [3 3 2 2 3 2 1 0 0 0 1 1 4 3 1 2 4 2 4 4]\n",
      "Cluster centers: [[0.01591402 0.01590982 0.01588203 0.93645662 0.01583751]\n",
      " [0.01163439 0.01163102 0.01164176 0.01164328 0.95344955]\n",
      " [0.0075776  0.96968875 0.00757843 0.00757944 0.00757578]\n",
      " [0.95217325 0.01206596 0.0119363  0.01194679 0.01187771]\n",
      " [0.02881533 0.02917447 0.88432597 0.02898559 0.02869864]]\n"
     ]
    }
   ],
   "source": [
    "# 用KM算法进行聚类，并提取关键词\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "K = 5\n",
    "\n",
    "# 创建KMeans实例，指定聚类数目K\n",
    "kmeans = KMeans(n_clusters=K)\n",
    "\n",
    "# 拟合模型\n",
    "kmeans.fit(lda_features)\n",
    "\n",
    "# 预测每个数据点的聚类标签\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# 获取聚类中心\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# 打印结果\n",
    "print(\"Cluster labels:\", labels)\n",
    "print(\"Cluster centers:\", centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb923db7-161c-4207-8b05-182875645e1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 主题词提炼区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "176164ec-bec4-466b-b172-5edccbea9fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 3 Topic Words: ['喜极而泣', '资格', '做空能', '曾经', '赚钱', '开户', '政策', '散户', '22', '预期']\n",
      "Category 2 Topic Words: ['知道', '股票', '接盘', '高位', '乞丐', '鸡翅', '行情', '转让', '办到', 'doge']\n",
      "Category 1 Topic Words: ['中国', '完全', '股市', '穷人', '始终', '赌场', '坚信', '般的', '贫富差距', '市场']\n",
      "Category 0 Topic Words: ['回调', '脱单', '技术性', '老王', '今天', '已经', '保住', '人里', '无论是', '关键在于']\n",
      "Category 4 Topic Words: ['恐惧', '贪婪', '不买', '笑哈哈', '祝福', '只会', '满仓', '调整', 'doge', '继续']\n"
     ]
    }
   ],
   "source": [
    "# 根据聚类的结果对主题词进行提炼\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# 为每个类别收集文档\n",
    "category_docs = {}\n",
    "for doc, label in zip(texts, labels):\n",
    "    if label not in category_docs:\n",
    "        category_docs[label] = []\n",
    "    category_docs[label].append(doc)\n",
    "\n",
    "# 初始化TF-IDF向量化器\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 将所有文档转换为TF-IDF向量\n",
    "texts_ = [\" \".join(t) for t in texts]\n",
    "X = vectorizer.fit_transform(texts_)\n",
    "\n",
    "# 初始化类别主题词字典\n",
    "category_topics = {}\n",
    "\n",
    "# 计算每个类别的文档数量\n",
    "category_doc_counts = {label: len(docs) for label, docs in category_docs.items()}\n",
    "\n",
    "# 计算每个类别的TF-IDF平均向量\n",
    "for label, docs in category_docs.items():\n",
    "    # 选择当前类别的所有文档的TF-IDF向量\n",
    "    category_tfidf = X.toarray()[labels == label]\n",
    "    \n",
    "    # 计算平均TF-IDF向量\n",
    "    category_avg_tfidf = category_tfidf.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    # 归一化平均TF-IDF向量,[0]将2D的值变成1D的\n",
    "    category_avg_tfidf = normalize(category_avg_tfidf, norm='l1')[0]\n",
    "    \n",
    "    # 存储类别的平均TF-IDF向量\n",
    "    category_topics[label] = category_avg_tfidf\n",
    "    \n",
    "# 获取特征名称（词汇）\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 为每个类别提取主题词\n",
    "def get_category_topic(label, num_words=10):\n",
    "    # 获取当前类别的平均TF-IDF向量\n",
    "    avg_tfidf = category_topics[label]\n",
    "    \n",
    "    # 获取词汇表中每个词的索引\n",
    "    # word_indices = avg_tfidf.indices\n",
    "    \n",
    "    # 获取每个词的TF-IDF值\n",
    "    # word_scores = avg_tfidf.data\n",
    "    word_scores = avg_tfidf\n",
    "    \n",
    "    # 按TF-IDF值降序排列词\n",
    "    sorted_indices = np.argsort(-word_scores)\n",
    "    \n",
    "    # 返回TF-IDF值最高的N个词作为主题词\n",
    "    top_n_words = [feature_names[index] for index in sorted_indices[:num_words]]\n",
    "    return top_n_words\n",
    "\n",
    "# 为每个类别提取主题词\n",
    "category_topic_words = {label: get_category_topic(label) for label in category_topics.keys()}\n",
    "\n",
    "for label, topic_words in category_topic_words.items():\n",
    "    print(f\"Category {label} Topic Words: {topic_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0572c8f7-fa7f-46e9-ab2e-23eb673b2bce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[323, 170, 324,  55, 122, 133, 266, 287, 126, 147, 270, 292,  90,\n",
       "         33, 294, 142, 239,   7,  91, 175, 322,  22, 111, 174, 203, 199,\n",
       "        191,  74, 247, 234, 196,  32, 220, 274, 100,  16,  95, 284, 260,\n",
       "        144, 137, 154, 136,  59,   6, 316, 172, 131, 315, 297,  23, 216,\n",
       "        228, 229, 280, 300,  37, 268, 235, 176,  54,  68,  21, 121, 190,\n",
       "        120, 113, 101, 299,  20, 112, 263, 312,  27, 262,  29, 149, 157,\n",
       "        161, 314,  56, 232, 317, 184,   0, 163, 251,  65,  85,  34, 240,\n",
       "        241, 242, 243, 244, 245, 246,  31, 248, 249, 250, 252, 253, 254,\n",
       "        255, 256, 257, 258, 259,  30, 261,  28, 238, 237, 236,  35, 208,\n",
       "        209, 210, 211, 212, 213, 214, 215,  42, 217, 218, 219,  41, 221,\n",
       "        222, 223, 224, 225, 226, 227,  40,  39, 230, 231,  38, 233,  36,\n",
       "         26, 264, 265,  25,  11, 298,  10,   9, 301, 302, 303, 304, 305,\n",
       "        306, 307, 308, 296, 309, 311,   8, 313,   5,   4,   3,   2, 318,\n",
       "        319, 320, 321,   1, 310, 207, 295, 293, 267,  24, 269,  19, 271,\n",
       "        272, 273,  18, 275, 276, 277, 278,  12, 279, 281, 282, 283,  15,\n",
       "        285, 286,  14, 288, 289, 290, 291,  13,  17, 206, 204,  84, 116,\n",
       "        117, 118, 119,  75,  73,  72, 123, 124, 125,  71, 127, 115, 128,\n",
       "        130,  70, 132,  69, 134, 135,  67,  66, 138, 139, 140, 141, 129,\n",
       "        143, 114,  77,  86,  87,  88,  89,  83,  82,  92,  93,  94,  81,\n",
       "         96,  97,  76,  98,  80,  79, 102, 103, 104, 105, 106, 107, 108,\n",
       "        109, 110,  78,  99, 205,  64, 146, 178, 179, 180, 181, 182, 183,\n",
       "         48, 185, 186, 187, 188, 189, 177,  47, 192, 193, 194, 195,  45,\n",
       "        197, 198,  44, 200, 201, 202,  43,  46, 145,  49,  51,  63, 148,\n",
       "         62, 150, 151, 152, 153,  61, 155, 156,  60, 158,  50, 159,  58,\n",
       "         57, 164, 165, 166, 167, 168, 169,  53, 171,  52, 173, 160, 162]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_scores = category_topics[2]\n",
    "\n",
    "sorted_indices = np.argsort(-word_scores)\n",
    "\n",
    "[feature_names[index] for index in sorted_indices[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da48f2-27c4-41c2-aa38-e0f3ff487642",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 指标计算区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1859f8e-92a3-49c4-8471-6dbf4526d3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮廓系数: 0.96\n"
     ]
    }
   ],
   "source": [
    "# sc指标\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(lda_features, labels)\n",
    "print(f\"轮廓系数: {silhouette_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96d21b-6051-4b29-9c64-dcbdddbea468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主题一致性 1\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "def calculate_pmi(word_counts, total_docs):\n",
    "    \"\"\"计算点互信息（PMI）\"\"\"\n",
    "    pmi = {}\n",
    "    for word1, word2 in combinations(word_counts, 2):\n",
    "        joint_prob = word_counts[(word1, word2)] / total_docs\n",
    "        p_word1 = word_counts[word1] / total_docs\n",
    "        p_word2 = word_counts[word2] / total_docs\n",
    "        pmi[(word1, word2)] = np.log(joint_prob / (p_word1 * p_word2)) if (p_word1 * p_word2) else 0\n",
    "    return pmi\n",
    "\n",
    "def u_mass_coherence(cluster, dictionary, total_docs):\n",
    "    \"\"\"计算U-Mass一致性\"\"\"\n",
    "    word_counts = {}\n",
    "    for doc in cluster:\n",
    "        for word1, word2 in combinations(set(doc), 2):\n",
    "            word_counts[(word1, word2)] = word_counts.get((word1, word2), 0) + 1\n",
    "    \n",
    "    pmi = calculate_pmi(word_counts, total_docs)\n",
    "    coherence_score = sum(pmi.values()) / len(pmi)\n",
    "    return coherence_score\n",
    "\n",
    "# 假设clusters是K-means聚类的结果，每个簇包含多个文档\n",
    "# dictionary是词汇表，total_docs是总文档数\n",
    "total_docs = len(documents)\n",
    "clusters = {...}  # 从K-means聚类结果中获取每个簇的文档\n",
    "\n",
    "# 计算每个簇的U-Mass一致性\n",
    "coherence_scores = {cluster_id: u_mass_coherence(cluster, dictionary, total_docs) for cluster_id, cluster in clusters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1bd328d-6630-408b-a7e8-1d62d3276e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 指标umass计算方式,如果采用UCI计算方式,把window_size的参数进行调整\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import combinations\n",
    "\n",
    "def build_co_occurrence_matrix(documents, window_size=2):\n",
    "    \"\"\"构建词共现矩阵\"\"\"\n",
    "    co_occurrence = defaultdict(int)\n",
    "    for doc in documents:\n",
    "        words = list(doc)\n",
    "        for i in range(len(words) - window_size + 1):\n",
    "            for pair in combinations(words[i:i + window_size], 2):\n",
    "                co_occurrence[pair] += 1\n",
    "    return co_occurrence\n",
    "\n",
    "def calculate_pmi(co_occurrence):\n",
    "    \"\"\"计算点互信息（PMI）\"\"\"\n",
    "    # 计算每个单独词的总出现次数\n",
    "    word_counts = Counter()\n",
    "    for pair, count in co_occurrence.items():\n",
    "        word_counts[pair[0]] += count\n",
    "        word_counts[pair[1]] += count\n",
    "\n",
    "    total_documents = sum(co_occurrence.values())\n",
    "\n",
    "    for pair, count in co_occurrence.items():\n",
    "        pair_prob = count * 2 / total_documents\n",
    "        word1, word2 = pair\n",
    "        word1_prob = word_counts[word1] / total_documents\n",
    "        word2_prob = word_counts[word2] / total_documents\n",
    "        pmi = np.log(pair_prob / (word1_prob * word2_prob)) if (word1_prob * word2_prob) > 0 else 0\n",
    "        co_occurrence[pair] = pmi\n",
    "\n",
    "    return co_occurrence\n",
    "\n",
    "def umass_score(texts, top_n=-1):\n",
    "    co_occurrence = build_co_occurrence_matrix(texts)\n",
    "    pmi_matrix = calculate_pmi(co_occurrence)\n",
    "    # 筛选出存在于PMI矩阵中的词对\n",
    "    # valid_pairs = [pair for pair in pmi_matrix.keys() if pmi_matrix[pair] > 0]\n",
    "    # top_pairs = sorted(valid_pairs, key=lambda x: pmi_matrix[x], reverse=True)[:top_n]\n",
    "    # umass = sum(pmi_matrix[pair] for pair in top_pairs) / len(top_pairs)\n",
    "    umass = sum(pmi_matrix.values())/len(pmi_matrix)\n",
    "    return umass\n",
    "\n",
    "\n",
    "coherence_scores = {label: umass_score(cluster) for label, cluster in category_docs.items()}\n",
    "average_coherence = sum(coherence_scores.values()) / len(coherence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75542d35-a91f-4ff0-88d1-ae89d13108d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 3.752682943892814,\n",
       " 2: 3.771461523474716,\n",
       " 1: 2.9060721031732024,\n",
       " 0: 3.1018875726822723,\n",
       " 4: 3.4052160307949904}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted([pair for pair in pmi_matrix.keys() if pmi_matrix[pair] > 0], key=lambda x: pmi_matrix[x], reverse=True)[:-1]\n",
    "coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e32dbfb-5fe9-40dd-be21-34d35edffa4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: ('apple', 'banana'), Co-occurrence count: 1\n",
      "Pair: ('banana', 'orange'), Co-occurrence count: 1\n",
      "Pair: ('orange', 'apple'), Co-occurrence count: 1\n",
      "Pair: ('apple', 'computer'), Co-occurrence count: 1\n",
      "Pair: ('computer', 'phone'), Co-occurrence count: 1\n",
      "Pair: ('phone', 'banana'), Co-occurrence count: 1\n",
      "Pair: ('banana', 'split'), Co-occurrence count: 1\n",
      "Pair: ('split', 'apple'), Co-occurrence count: 1\n",
      "Pair: ('apple', 'grape'), Co-occurrence count: 1\n"
     ]
    }
   ],
   "source": [
    "# word2vec指标计算方式英文预料区,需进行一定程度修改\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 假设topics_terms是一个包含主题和对应词的列表\n",
    "# 例如：topics_terms = [('topic1', ['word1', 'word2', 'word3']), ('topic2', ['word4', 'word5', 'word6'])]\n",
    "topics_terms = [...]  # 你的LDA主题模型的结果\n",
    "\n",
    "# 准备语料库数据，用于训练Word2Vec模型\n",
    "all_words = [term for _, terms in topics_terms for term in terms]\n",
    "dictionary = Dictionary(all_words)\n",
    "corpus = [dictionary.doc2bow(term) for _, terms in topics_terms for term in terms]\n",
    "\n",
    "# 训练Word2Vec模型\n",
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 函数：计算主题的一致性得分\n",
    "def calculate_coherence_score(topic_words, model):\n",
    "    # 将词转换为向量\n",
    "    vectors = np.array([model.wv[word] for word in topic_words if word in model.wv])\n",
    "    \n",
    "    # 计算词向量之间的余弦相似度\n",
    "    if len(vectors) > 1:\n",
    "        similarities = cosine_similarity(vectors)\n",
    "        return np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 计算每个主题的一致性得分\n",
    "coherence_scores = {}\n",
    "for topic_id, terms in topics_terms:\n",
    "    score = calculate_coherence_score(terms, model)\n",
    "    coherence_scores[topic_id] = score\n",
    "    print(f\"Topic '{topic_id}' coherence score: {score}\")\n",
    "\n",
    "# 可以选择输出平均一致性得分\n",
    "average_coherence_score = sum(coherence_scores.values()) / len(coherence_scores)\n",
    "print(f\"Average coherence score: {average_coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36ff6287-12d2-43f1-9eb1-a5ba94064e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jc散度->无意义\n",
    "def jaccard_distance(u, v):\n",
    "    intersection = len(set(u) & set(v))\n",
    "    union = len(set(u) | set(v))\n",
    "    return 1 - intersection / union if union != 0 else 0\n",
    "\n",
    "def jaccard_coefficient(centers):\n",
    "    n_clusters = len(centers)\n",
    "    jaccard_sum = 0\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(i + 1, n_clusters):\n",
    "            distance = jaccard_distance(centers[i], centers[j])\n",
    "            jaccard_sum += distance\n",
    "    \n",
    "    return jaccard_sum / (n_clusters * (n_clusters - 1) / 2)\n",
    "\n",
    "# 示例使用\n",
    "# centers = np.array([...])  # 聚类中心\n",
    "# jc = jaccard_coefficient(centers)\n",
    "# print(\"Jaccard 散度:\", jc)\n",
    "jaccard_coefficient(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5e0d1-81b4-459f-94ce-15efae331f32",
   "metadata": {},
   "source": [
    "# 其余事物试验场"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e87c313-c0c2-4b80-9725-ae78fecedaae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 3331.32it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[22, 年, 入市, 昨天早上, 最高, 位, 离场, 达到, 预期, 收益, 收手, 纪...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[今天, 已经, 暴跌, 认为, 下午, 国家队, 这才, 第二天, 砸, 盘, 猛, 主力...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[昨天, 店, 内, 一边, 吃, 鸡翅, 一边, 手机, 看, 股票, 乞丐, 乞讨, 一...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[几天, 大阳线, 股民, 以前, 记忆, 都, 干, 没, doge, 根本, 不, 知道...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[经历, 2008, 年, 股灾, 老人, 回忆, 股市, 政策底, 市场, 底, 政策, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[特别, 微观, 直观, 例子, 看下, 银行, 大额, 存单, 转让, 区, 都, 居民,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[始终, 坚信, 赌场, 般的, 市场, 大多数, 人, 赚, 钱]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[今天, 已经, 技术性, 回调, 脱单, doge]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[都, 看涨, 导致, 目前, 赛道, 拥挤, 老王, 直接, 看空, 以此, 安慰, 没,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[无论是, 涨, 跌, 最终, 赚, 保住, 钱, 人, 都, 极少数人, 关键在于, 清晰...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[重申, 一下, 股市, 穷人, 最少, 地方, 中产, 最多, 地方, 股市, 放水, 造...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[记住, 中国, 玩, 股票, 人, 不到, 10, 个点, 中国, 股市, 反应, 中国,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[涨, 买, 跌, 只会, 笑哈哈]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[喜极而泣, 曾经, 做空能, 赚钱, 开户, 资格, 都]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[这种, 完全, 不, 关注, 圈子, 股票, 完全, 没, 交集, 人, 刷, 股市, 视...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[几句话, 精辟, 拜佛, 有用, 庙, 门, 都, 进不去, 种地, 致富, 农民, 无地...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[不, 懂, 不买, 祝福]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[参照, 519, 行情, 最大, 区别, 高位, 接盘, 真的, 解套, 现在, 高位, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[恐惧, 贪婪, 满仓, doge]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[大多数, 散户, 牛市, 中, 赚, 钱, 人, 克服, 人性, 贪婪, 恐惧, 总, 想...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            documents  label\n",
       "0   [22, 年, 入市, 昨天早上, 最高, 位, 离场, 达到, 预期, 收益, 收手, 纪...      1\n",
       "1   [今天, 已经, 暴跌, 认为, 下午, 国家队, 这才, 第二天, 砸, 盘, 猛, 主力...      1\n",
       "2   [昨天, 店, 内, 一边, 吃, 鸡翅, 一边, 手机, 看, 股票, 乞丐, 乞讨, 一...      4\n",
       "3   [几天, 大阳线, 股民, 以前, 记忆, 都, 干, 没, doge, 根本, 不, 知道...      1\n",
       "4   [经历, 2008, 年, 股灾, 老人, 回忆, 股市, 政策底, 市场, 底, 政策, ...      4\n",
       "5   [特别, 微观, 直观, 例子, 看下, 银行, 大额, 存单, 转让, 区, 都, 居民,...      0\n",
       "6                  [始终, 坚信, 赌场, 般的, 市场, 大多数, 人, 赚, 钱]      4\n",
       "7                         [今天, 已经, 技术性, 回调, 脱单, doge]      1\n",
       "8   [都, 看涨, 导致, 目前, 赛道, 拥挤, 老王, 直接, 看空, 以此, 安慰, 没,...      0\n",
       "9   [无论是, 涨, 跌, 最终, 赚, 保住, 钱, 人, 都, 极少数人, 关键在于, 清晰...      2\n",
       "10  [重申, 一下, 股市, 穷人, 最少, 地方, 中产, 最多, 地方, 股市, 放水, 造...      3\n",
       "11  [记住, 中国, 玩, 股票, 人, 不到, 10, 个点, 中国, 股市, 反应, 中国,...      4\n",
       "12                                 [涨, 买, 跌, 只会, 笑哈哈]      0\n",
       "13                     [喜极而泣, 曾经, 做空能, 赚钱, 开户, 资格, 都]      1\n",
       "14  [这种, 完全, 不, 关注, 圈子, 股票, 完全, 没, 交集, 人, 刷, 股市, 视...      2\n",
       "15  [几句话, 精辟, 拜佛, 有用, 庙, 门, 都, 进不去, 种地, 致富, 农民, 无地...      0\n",
       "16                                     [不, 懂, 不买, 祝福]      0\n",
       "17  [参照, 519, 行情, 最大, 区别, 高位, 接盘, 真的, 解套, 现在, 高位, ...      2\n",
       "18                                 [恐惧, 贪婪, 满仓, doge]      1\n",
       "19  [大多数, 散户, 牛市, 中, 赚, 钱, 人, 克服, 人性, 贪婪, 恐惧, 总, 想...      2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bitermplus as btm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PREPROCESSING\n",
    "# Obtaining terms frequency in a sparse matrix and corpus vocabulary\n",
    "X, vocabulary, vocab_dict = btm.get_words_freqs(texts_)\n",
    "tf = np.array(X.sum(axis=0)).ravel()\n",
    "# Vectorizing documents\n",
    "docs_vec = btm.get_vectorized_docs(texts_, vocabulary)\n",
    "docs_lens = list(map(len, docs_vec))\n",
    "# Generating biterms\n",
    "biterms = btm.get_biterms(docs_vec)\n",
    "\n",
    "# INITIALIZING AND RUNNING MODEL\n",
    "model = btm.BTM(\n",
    "    X, vocabulary, seed=12321, T=5, M=20, alpha=50/8, beta=0.01)\n",
    "model.fit(biterms, iterations=20)\n",
    "# p_zd就是btm_features\n",
    "p_zd = model.transform(docs_vec)\n",
    "\n",
    "# METRICS\n",
    "perplexity = btm.perplexity(model.matrix_topics_words_, p_zd, X, 8)\n",
    "coherence = btm.coherence(model.matrix_topics_words_, X, M=20)\n",
    "# or\n",
    "perplexity = model.perplexity_\n",
    "coherence = model.coherence_\n",
    "\n",
    "# LABELS\n",
    "model.labels_\n",
    "# or\n",
    "btm.get_docs_top_topic(texts, model.matrix_docs_topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3c44161-23c9-443b-9574-cbd69b08d3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89.41598629, -23.31997517,          inf, 108.47268635,\n",
       "                inf])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdb332-80eb-4909-80fc-d52db7868507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[feature_names[index] for index in sorted_indices[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80f516cd-1a7a-4c8e-98f6-86256f23e651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4090901  0.57496187 0.4090901  0.         0.4090901  0.4090901 ]\n",
      " [0.5779446  0.         0.2889723  0.40614049 0.5779446  0.2889723 ]]\n",
      "['document' 'first' 'is' 'second' 'the' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "# 创建 CountVectorizer 对象\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 训练 CountVectorizer 并转换数据\n",
    "X = [\"this is the first document\", \"this document is the second document the\"]\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# 获取词汇表\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 将稀疏矩阵转换为数组\n",
    "X_array = X_vectorized.toarray()\n",
    "\n",
    "# 打印结果\n",
    "print(X_array)\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b42311d5-8822-4087-9729-b8f43d8d22f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0 Top Words: ['document', 'first', 'is', 'the', 'this']\n",
      "Category 1 Top Words: ['document', 'is', 'second', 'the', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# 假设documents是一个文档列表，labels是对应的类别标签\n",
    "documents = X\n",
    "labels = [0, 1]\n",
    "\n",
    "# 为每个类别收集文档\n",
    "category_docs = {}\n",
    "for doc, label in zip(documents, labels):\n",
    "    if label not in category_docs:\n",
    "        category_docs[label] = []\n",
    "    category_docs[label].append(doc)\n",
    "\n",
    "\n",
    "# 为所有文档构建词汇表\n",
    "vectorizer.fit(documents)\n",
    "\n",
    "# 对每个类别的文档进行向量化，并统计词频\n",
    "category_word_counts = {}\n",
    "for category, docs in category_docs.items():\n",
    "    # 对当前类别的文档进行向量化\n",
    "    bow = vectorizer.transform(docs).toarray()\n",
    "    # 获取词汇表中每个词的列索引\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    # 计算每个词的总词频\n",
    "    word_freq = bow.sum(axis=0).tolist()\n",
    "    # 创建一个计数器对象，统计每个词的频率\n",
    "    category_word_counts[category] = Counter(\n",
    "        [feature_names[i] for i, freq in enumerate(word_freq) if freq > 0]\n",
    "    )\n",
    "\n",
    "# 选择每个类别的高频词\n",
    "top_words_per_category = {}\n",
    "for category, word_counts in category_word_counts.items():\n",
    "    top_words = word_counts.most_common(10)  # 选择前10个高频词\n",
    "    top_words_per_category[category] = [word for word, _ in top_words]\n",
    "\n",
    "# 打印每个类别的主题词\n",
    "for category, top_words in top_words_per_category.items():\n",
    "    print(f\"Category {category} Top Words: {top_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea2732b5-efd2-4a77-a653-3dbd49defd22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Counter({'document': 1, 'first': 1, 'is': 1, 'the': 1, 'this': 1}),\n",
       " 1: Counter({'document': 1, 'is': 1, 'second': 1, 'the': 1, 'this': 1})}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8831a92f-98e2-401b-86e4-e190ea7cc05d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\tool\\miniconda3\\envs\\allInOne\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "D:\\tool\\miniconda3\\envs\\allInOne\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels: [2 2 0 1 1 1]\n",
      "Cluster centers: [[1. 0.]\n",
      " [4. 2.]\n",
      " [1. 3.]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "349e1e84-7327-4587-88ff-ec55c4b63234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1)], [(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(3, 1), (5, 1), (9, 2), (14, 2), (15, 3), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2), (24, 2), (25, 3)], [(2, 1), (3, 2), (20, 1), (23, 1), (26, 1), (27, 1), (28, 1), (29, 3), (30, 3)], [(0, 1), (7, 1), (16, 1), (17, 1), (31, 1), (32, 1), (33, 1), (34, 1)], [(4, 1), (20, 1), (30, 1), (35, 1), (36, 1), (37, 2), (38, 1), (39, 2)], [(12, 1), (13, 1), (32, 1), (33, 1), (40, 1)], [(2, 1), (5, 1), (6, 1)], [(11, 1), (20, 1), (28, 1), (30, 1), (41, 1), (42, 2)], [(3, 1), (12, 1), (13, 1), (21, 1), (30, 1), (32, 1), (43, 1), (44, 1)], [(13, 1), (19, 1), (28, 1), (34, 3), (35, 1), (41, 1), (43, 1), (45, 3), (46, 6), (47, 2), (48, 2), (49, 2), (50, 4), (51, 4), (52, 2), (53, 6), (54, 2), (55, 5)], [(11, 1), (23, 1), (28, 1), (32, 1), (34, 1), (35, 1), (56, 3)], [(21, 1), (44, 1)], [(27, 1), (30, 1), (57, 1)], [(3, 1), (20, 1), (23, 1), (32, 1), (34, 1), (58, 2)], [(2, 1), (23, 1), (30, 2), (57, 1)], [(3, 1), (18, 1)], [(38, 1), (59, 2), (60, 2)], [(1, 1), (2, 1), (61, 1), (62, 1)], [(1, 6), (3, 2), (6, 1), (8, 1), (10, 1), (12, 1), (13, 1), (16, 1), (20, 2), (22, 4), (26, 1), (30, 2), (31, 2), (32, 2), (34, 1), (36, 1), (40, 1), (44, 1), (61, 5), (62, 1), (63, 2), (64, 2), (65, 3), (66, 2), (67, 2), (68, 4), (69, 2), (70, 2), (71, 2), (72, 5), (73, 2), (74, 3), (75, 6), (76, 2), (77, 2)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cb0fabf-b85a-4202-a9f5-b3e9760d465e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['22',\n",
       "  '年',\n",
       "  '入市',\n",
       "  '昨天早上',\n",
       "  '最高',\n",
       "  '位',\n",
       "  '离场',\n",
       "  '达到',\n",
       "  '预期',\n",
       "  '收益',\n",
       "  '收手',\n",
       "  '纪律性',\n",
       "  '对抗',\n",
       "  '贪婪',\n",
       "  '本性',\n",
       "  '唯一',\n",
       "  '办法',\n",
       "  'OK'],\n",
       " ['今天',\n",
       "  '已经',\n",
       "  '暴跌',\n",
       "  '认为',\n",
       "  '下午',\n",
       "  '国家队',\n",
       "  '这才',\n",
       "  '第二天',\n",
       "  '砸',\n",
       "  '盘',\n",
       "  '猛',\n",
       "  '主力',\n",
       "  '有点',\n",
       "  '不',\n",
       "  '给面子',\n",
       "  '盲猜',\n",
       "  '收盘',\n",
       "  '之前',\n",
       "  '翻红',\n",
       "  '散户',\n",
       "  '看',\n",
       "  '明白',\n",
       "  '政策',\n",
       "  '经济',\n",
       "  '复苏',\n",
       "  '那套',\n",
       "  '玩意',\n",
       "  '散户',\n",
       "  '钱',\n",
       "  '盲目',\n",
       "  '入坑',\n",
       "  '冷静',\n",
       "  'doge',\n",
       "  '_',\n",
       "  '金箍',\n",
       "  '反正',\n",
       "  '没入',\n",
       "  '不亏',\n",
       "  '赚'],\n",
       " ['昨天',\n",
       "  '店',\n",
       "  '内',\n",
       "  '一边',\n",
       "  '吃',\n",
       "  '鸡翅',\n",
       "  '一边',\n",
       "  '手机',\n",
       "  '看',\n",
       "  '股票',\n",
       "  '乞丐',\n",
       "  '乞讨',\n",
       "  '一块',\n",
       "  '鸡翅',\n",
       "  '后',\n",
       "  '继续',\n",
       "  '看',\n",
       "  '股票',\n",
       "  '乞丐',\n",
       "  '啃着',\n",
       "  '鸡翅',\n",
       "  '没',\n",
       "  '走',\n",
       "  '一旁',\n",
       "  '看着',\n",
       "  '说',\n",
       "  '长期',\n",
       "  '均线',\n",
       "  '金叉',\n",
       "  'KDJ',\n",
       "  '数值',\n",
       "  '底部',\n",
       "  '反复',\n",
       "  '钝化',\n",
       "  'MACD',\n",
       "  '底',\n",
       "  '背离',\n",
       "  '能量',\n",
       "  '潮',\n",
       "  '喇叭口',\n",
       "  '扩大',\n",
       "  '这股',\n",
       "  '涨',\n",
       "  '惊诧',\n",
       "  '地问',\n",
       "  '懂',\n",
       "  '乞丐',\n",
       "  '说',\n",
       "  '不',\n",
       "  '懂',\n",
       "  '今天'],\n",
       " ['几天',\n",
       "  '大阳线',\n",
       "  '股民',\n",
       "  '以前',\n",
       "  '记忆',\n",
       "  '都',\n",
       "  '干',\n",
       "  '没',\n",
       "  'doge',\n",
       "  '根本',\n",
       "  '不',\n",
       "  '知道',\n",
       "  '疯狂',\n",
       "  '接触',\n",
       "  '股票',\n",
       "  '朋友',\n",
       "  '都',\n",
       "  '开户',\n",
       "  '问',\n",
       "  '玩',\n",
       "  '知道',\n",
       "  '板块',\n",
       "  '都',\n",
       "  '找',\n",
       "  '股票代码',\n",
       "  '沪',\n",
       "  '深',\n",
       "  '不',\n",
       "  '知道',\n",
       "  '投',\n",
       "  '几万块'],\n",
       " ['经历',\n",
       "  '2008',\n",
       "  '年',\n",
       "  '股灾',\n",
       "  '老人',\n",
       "  '回忆',\n",
       "  '股市',\n",
       "  '政策底',\n",
       "  '市场',\n",
       "  '底',\n",
       "  '政策',\n",
       "  '到位',\n",
       "  '上涨',\n",
       "  '一轮',\n",
       "  '引',\n",
       "  '人',\n",
       "  '下场',\n",
       "  '后',\n",
       "  '再',\n",
       "  '跌停',\n",
       "  '逃生',\n",
       "  '巨亏',\n",
       "  '40%',\n",
       "  '狠狠心',\n",
       "  '抛',\n",
       "  '逃过',\n",
       "  '后面',\n",
       "  '下跌'],\n",
       " ['特别',\n",
       "  '微观',\n",
       "  '直观',\n",
       "  '例子',\n",
       "  '看下',\n",
       "  '银行',\n",
       "  '大额',\n",
       "  '存单',\n",
       "  '转让',\n",
       "  '区',\n",
       "  '都',\n",
       "  '居民',\n",
       "  '存款',\n",
       "  '节前',\n",
       "  '转让',\n",
       "  '收益率',\n",
       "  '上升',\n",
       "  '10',\n",
       "  '20bp',\n",
       "  '说明',\n",
       "  '这轮',\n",
       "  '行情',\n",
       "  '真真实实',\n",
       "  '撬动',\n",
       "  '银行存款',\n",
       "  '之前',\n",
       "  'zc',\n",
       "  '没',\n",
       "  '办到',\n",
       "  '事情',\n",
       "  '这次',\n",
       "  '办到'],\n",
       " ['始终', '坚信', '赌场', '般的', '市场', '大多数', '人', '赚', '钱'],\n",
       " ['今天', '已经', '技术性', '回调', '脱单', 'doge'],\n",
       " ['都',\n",
       "  '看涨',\n",
       "  '导致',\n",
       "  '目前',\n",
       "  '赛道',\n",
       "  '拥挤',\n",
       "  '老王',\n",
       "  '直接',\n",
       "  '看空',\n",
       "  '以此',\n",
       "  '安慰',\n",
       "  '没',\n",
       "  '上车',\n",
       "  '群体',\n",
       "  '别出心裁',\n",
       "  '另辟蹊径',\n",
       "  '看似',\n",
       "  '谈',\n",
       "  '经济',\n",
       "  '实则',\n",
       "  '玩',\n",
       "  '媒体',\n",
       "  '老王',\n",
       "  '高',\n",
       "  '支持'],\n",
       " ['无论是',\n",
       "  '涨',\n",
       "  '跌',\n",
       "  '最终',\n",
       "  '赚',\n",
       "  '保住',\n",
       "  '钱',\n",
       "  '人',\n",
       "  '都',\n",
       "  '极少数人',\n",
       "  '关键在于',\n",
       "  '清晰',\n",
       "  '认识',\n",
       "  '人里',\n",
       "  '面包',\n",
       "  '不',\n",
       "  '包括'],\n",
       " ['重申',\n",
       "  '一下',\n",
       "  '股市',\n",
       "  '穷人',\n",
       "  '最少',\n",
       "  '地方',\n",
       "  '中产',\n",
       "  '最多',\n",
       "  '地方',\n",
       "  '股市',\n",
       "  '放水',\n",
       "  '造成',\n",
       "  '中产',\n",
       "  '变成',\n",
       "  '富人',\n",
       "  '穷人',\n",
       "  '穷人',\n",
       "  'Doge',\n",
       "  '动动',\n",
       "  '猪脑子',\n",
       "  '想想',\n",
       "  '穷人',\n",
       "  '钱',\n",
       "  '玩',\n",
       "  '万儿八千',\n",
       "  '好点',\n",
       "  '十万八万',\n",
       "  '顶天',\n",
       "  '富人',\n",
       "  '百万',\n",
       "  '千万',\n",
       "  '股市',\n",
       "  '翻',\n",
       "  '一倍',\n",
       "  '富人',\n",
       "  '100',\n",
       "  '万变',\n",
       "  '200',\n",
       "  '万',\n",
       "  '穷人',\n",
       "  '5',\n",
       "  '万变',\n",
       "  '10',\n",
       "  '万',\n",
       "  '贫富差距',\n",
       "  '增加',\n",
       "  '最终',\n",
       "  '贫富差距',\n",
       "  '初始',\n",
       "  '贫富差距',\n",
       "  '190',\n",
       "  '万',\n",
       "  '95',\n",
       "  '万',\n",
       "  '95',\n",
       "  '万',\n",
       "  '贫富差距',\n",
       "  '增加',\n",
       "  '95',\n",
       "  '万',\n",
       "  '表明',\n",
       "  '穷人',\n",
       "  '财富',\n",
       "  '增加',\n",
       "  '富人',\n",
       "  '财富',\n",
       "  '增加',\n",
       "  '比例',\n",
       "  '更大',\n",
       "  '导致',\n",
       "  '贫富差距',\n",
       "  '扩大',\n",
       "  '告诉',\n",
       "  '放水'],\n",
       " ['记住',\n",
       "  '中国',\n",
       "  '玩',\n",
       "  '股票',\n",
       "  '人',\n",
       "  '不到',\n",
       "  '10',\n",
       "  '个点',\n",
       "  '中国',\n",
       "  '股市',\n",
       "  '反应',\n",
       "  '中国',\n",
       "  '经济',\n",
       "  'A股',\n",
       "  '赌字',\n",
       "  '指望',\n",
       "  '分析'],\n",
       " ['涨', '买', '跌', '只会', '笑哈哈'],\n",
       " ['喜极而泣', '曾经', '做空能', '赚钱', '开户', '资格', '都'],\n",
       " ['这种',\n",
       "  '完全',\n",
       "  '不',\n",
       "  '关注',\n",
       "  '圈子',\n",
       "  '股票',\n",
       "  '完全',\n",
       "  '没',\n",
       "  '交集',\n",
       "  '人',\n",
       "  '刷',\n",
       "  '股市',\n",
       "  '视频',\n",
       "  '....',\n",
       "  '微笑'],\n",
       " ['几句话',\n",
       "  '精辟',\n",
       "  '拜佛',\n",
       "  '有用',\n",
       "  '庙',\n",
       "  '门',\n",
       "  '都',\n",
       "  '进不去',\n",
       "  '种地',\n",
       "  '致富',\n",
       "  '农民',\n",
       "  '无地',\n",
       "  '可种',\n",
       "  '股票',\n",
       "  '赚钱',\n",
       "  '连证',\n",
       "  '劵',\n",
       "  '账户',\n",
       "  '都',\n",
       "  '开',\n",
       "  '社会',\n",
       "  '基本规律',\n",
       "  'doge'],\n",
       " ['不', '懂', '不买', '祝福'],\n",
       " ['参照',\n",
       "  '519',\n",
       "  '行情',\n",
       "  '最大',\n",
       "  '区别',\n",
       "  '高位',\n",
       "  '接盘',\n",
       "  '真的',\n",
       "  '解套',\n",
       "  '现在',\n",
       "  '高位',\n",
       "  '接盘',\n",
       "  '套',\n",
       "  '一辈子'],\n",
       " ['恐惧', '贪婪', '满仓', 'doge'],\n",
       " ['大多数',\n",
       "  '散户',\n",
       "  '牛市',\n",
       "  '中',\n",
       "  '赚',\n",
       "  '钱',\n",
       "  '人',\n",
       "  '克服',\n",
       "  '人性',\n",
       "  '贪婪',\n",
       "  '恐惧',\n",
       "  '总',\n",
       "  '想着',\n",
       "  '逃顶',\n",
       "  '事实上',\n",
       "  '次',\n",
       "  '机会',\n",
       "  '都',\n",
       "  '逃不了',\n",
       "  '举个',\n",
       "  '例子',\n",
       "  '3000',\n",
       "  '点',\n",
       "  '买入',\n",
       "  '大盘',\n",
       "  '连续',\n",
       "  '疯涨',\n",
       "  '3600',\n",
       "  '点',\n",
       "  '浮盈',\n",
       "  '20%',\n",
       "  '这时候',\n",
       "  '几根',\n",
       "  '大',\n",
       "  '阴线',\n",
       "  '调整',\n",
       "  '连忙',\n",
       "  '卖出',\n",
       "  '损失',\n",
       "  '5%',\n",
       "  '利润',\n",
       "  '调整',\n",
       "  '几天',\n",
       "  '后',\n",
       "  '继续',\n",
       "  '涨到',\n",
       "  '3700',\n",
       "  '点',\n",
       "  '贪婪',\n",
       "  '满仓',\n",
       "  '一路',\n",
       "  '涨到',\n",
       "  '4000',\n",
       "  '点',\n",
       "  '新一轮',\n",
       "  '调整',\n",
       "  '上次',\n",
       "  '教训',\n",
       "  '轻易',\n",
       "  '抛出',\n",
       "  '筹码',\n",
       "  '调整',\n",
       "  '3800',\n",
       "  '点后',\n",
       "  '继续',\n",
       "  '上涨',\n",
       "  '4200',\n",
       "  '点',\n",
       "  '这时候',\n",
       "  '手里',\n",
       "  '仅',\n",
       "  '子弹',\n",
       "  '借钱',\n",
       "  '融资',\n",
       "  '大盘',\n",
       "  '上涨',\n",
       "  '4300',\n",
       "  '点后',\n",
       "  '调整',\n",
       "  '维持',\n",
       "  '上次',\n",
       "  '判断',\n",
       "  '良性',\n",
       "  '调整',\n",
       "  '大幅',\n",
       "  '下',\n",
       "  '杀',\n",
       "  '加仓',\n",
       "  '个股',\n",
       "  '亏损',\n",
       "  '仅仅',\n",
       "  '一天',\n",
       "  '利润',\n",
       "  '没',\n",
       "  '一半',\n",
       "  '心态',\n",
       "  '崩',\n",
       "  '不',\n",
       "  '认输',\n",
       "  '觉得',\n",
       "  '卖出去',\n",
       "  '彻底',\n",
       "  '没',\n",
       "  '希望',\n",
       "  '第二天',\n",
       "  '开盘',\n",
       "  '继续',\n",
       "  '杀',\n",
       "  '觉得',\n",
       "  '已经',\n",
       "  '大跌',\n",
       "  '两天',\n",
       "  '明天',\n",
       "  '反弹',\n",
       "  '逃',\n",
       "  '第三天',\n",
       "  '继续',\n",
       "  '跌',\n",
       "  '利润',\n",
       "  '全没',\n",
       "  '所有人',\n",
       "  '都',\n",
       "  '觉得',\n",
       "  '贪婪',\n",
       "  '时',\n",
       "  '恐惧',\n",
       "  '恐惧',\n",
       "  '时',\n",
       "  '贪婪',\n",
       "  '事实上',\n",
       "  '贪婪',\n",
       "  '时',\n",
       "  '更',\n",
       "  '贪婪',\n",
       "  '恐惧',\n",
       "  '时',\n",
       "  '不',\n",
       "  '更',\n",
       "  '恐惧',\n",
       "  '股市',\n",
       "  '里',\n",
       "  '改变',\n",
       "  '人',\n",
       "  '只',\n",
       "  '需要',\n",
       "  '三根',\n",
       "  '大',\n",
       "  '阴线']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (trainingField)",
   "language": "python",
   "name": "pycharm-d26ea172"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
