{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c19e14d",
   "metadata": {},
   "source": [
    "# **Trial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc615ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:37:02.399222Z",
     "start_time": "2024-05-03T06:37:01.130549Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora, models\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34d215f-8c76-4dfd-a077-0b03284c4ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22年入市，昨天早上几乎最高位离场，达到预期收益就收手，纪律性是对抗贪婪本性的唯一办法[OK]',\n",
       " '今天已经暴跌了，但是我认为下午会有国家队。。。这才第二天就砸盘这么猛，主力有点不给面子了，盲猜收盘之前会翻红，但是也让散户看明白了，什么政策什么经济复苏，还是那套玩意，就是嘎散户的钱，盲目入坑的绝对冷静了[doge_金箍]反正我是没入，不亏就是赚了',\n",
       " '昨天在店内一边吃鸡翅一边用手机看股票。一个乞丐进来乞讨，我给他一块鸡翅后继续看股票。乞丐啃着鸡翅没走，也在一旁看着，他说：“长期均线金叉，KDJ数值底部反复钝化，MACD底背离，能量潮喇叭口扩大，这股要涨了。”我惊诧地问：“这个你也懂？” 乞丐说：“不懂我能有今天？”',\n",
       " '几天的大阳线把股民们以前的记忆都干没了[doge]，你们根本不知道有多疯狂，我一个从未接触过股票的朋友都开户来问我怎么玩了，你可知道他竟然连板块都不会找股票代码什么是沪深的也不知道就投了几万块进去',\n",
       " '经历2008年股灾的老人回忆到，股市有政策底，还有市场底。政策到位，上涨一轮，引人下场后再跌停，让你来不及逃生。我巨亏40%，狠狠心抛了，逃过了后面的下跌。',\n",
       " '一个特别微观又直观的例子：大家可以看下各银行的大额存单转让区，这里基本都是居民存款，节前转让收益率上升了10-20bp。说明这轮行情真真实实的撬动了银行存款，之前那么多zc没办到的事情这次竟然办到了。',\n",
       " '我始终坚信，一个赌场般的市场，绝对不会让大多数人赚到钱。',\n",
       " '今天已经开始技术性回调了[脱单doge]',\n",
       " '别人都是看涨，导致目前赛道过于拥挤。而老王直接看空，以此来安慰没上车的群体，别出心裁、另辟蹊径。看似在谈经济，实则在玩自媒体。老王高啊[支持]',\n",
       " '无论是涨是跌，最终能赚到并且保住这些钱的人都是极少数人，关键在于能不能清晰地认识到在这些人里面包不包括自己。',\n",
       " '我重申一下：股市是穷人最少的地方，中产最多的地方。\\n在股市放水，造成的结果就是，中产变成富人，穷人还是穷人。\\n【Doge】动动猪脑子想想，穷人能有多少钱玩？万儿八千，好点的十万八万，顶天了。\\n富人可是百万千万。\\n股市翻一倍，富人100万变200万，穷人5万变10万。\\n\\n 贫富差距增加了 = 最终贫富差距 - 初始贫富差距 = 190万 - 95万 = 95万\\n\\n因此。\\n贫富差距增加了95万！！这表明，尽管穷人的财富也增加了，但富人财富的增加比例更大，导致贫富差距扩大，你告诉我这是在放水？',\n",
       " '记住，中国玩股票的人，不到10个点。\\n所以中国股市从来就不能反应中国经济。\\nA股从来就是一个赌字。\\n别指望分析有什么用',\n",
       " '你涨，我也不会买，你跌，我只会笑哈哈',\n",
       " '[喜极而泣]曾经做空能赚钱，我连开户资格都没有',\n",
       " '当我这种完全不关注、圈子和股票完全没交集的人也刷到了股市的视频....[微笑]',\n",
       " '最后几句话精辟，如果拜佛有用，你庙门都进不去。如果种地能致富，那农民将无地可种。如果股票能赚钱，你连证劵账户都开不了。社会的基本规律[doge]',\n",
       " '不懂，不买，祝福。',\n",
       " '别参照519行情，一个最大的区别就是那时就算在高位接盘了后来也是真的能解套，现在要是高位接盘那就套一辈子',\n",
       " '别人恐惧我贪婪，满仓！[doge]',\n",
       " '为什么大多数散户在牛市中你也赚不了钱？因为你是人，你克服不了人性的贪婪和恐惧，你总想着你能逃顶，事实上给你多少次机会，你都逃不了。\\n举个例子：你在3000点买入，接下来大盘连续疯涨到3600点，你浮盈大概20%，这时候来几根大阴线调整，你连忙卖出，让你损失了5%的利润。调整几天后，继续涨到3700点，因为贪婪，你又满仓进来，接下来一路涨到4000点。开始新一轮调整，有了上次的教训，你不再轻易抛出筹码，果然，调整到3800点后继续上涨到4200点，这时候你把手里的仅有的子弹也打了进去，甚至开始借钱融资。接下来大盘上涨到4300点后开始开始调整，你还是维持你上次的判断，只是良性调整，结果开始大幅下杀，你因为一直加仓，有些个股甚至开始亏损，仅仅一天让你的利润没了一半，你心态崩了，你不认输，因为你觉得卖出去了就彻底没希望了，然后第二天开盘继续杀，你觉得已经大跌两天了，等明天反弹了你就逃，结果第三天继续跌，你的利润全没了。\\n\\n所有人都觉得别人贪婪时我恐惧，别人恐惧时我贪婪，事实上别人贪婪时你更贪婪，别人恐惧时不更恐惧。\\n\\n在股市里改变一个人，只需要三根大阴线。']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\stock\\BV1LuxZeVE25.json'\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as file:\n",
    "    # 加载 JSON 数据\n",
    "    data = json.load(file)\n",
    "    \n",
    "doc_data = [info['review'] for info in data]\n",
    "doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd85988-a1a6-4c28-99ef-df85d9c577b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\11435\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.504 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 读取停用词，并去停用词\n",
    "stopwords_path1 = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\corpus\\stopwords_scu.txt'\n",
    "with open(stopwords_path1, 'r', encoding='utf-8') as f:\n",
    "    stopwords1 = set([line.strip() for line in f])\n",
    "\n",
    "stopwords_path2 = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\corpus\\stopwords_hit.txt'\n",
    "with open(stopwords_path2, 'r', encoding='utf-8') as f:\n",
    "    stopwords2 = set([line.strip() for line in f])\n",
    "\n",
    "stopwords = stopwords1.union(stopwords2)\n",
    "\n",
    "texts = []\n",
    "for doc in doc_data:\n",
    "    words = jieba.cut(doc)\n",
    "    filter_words = [word for word in words if word not in stopwords and word.strip() != '']\n",
    "    texts.append(filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ae832eb-7c6c-48e3-9edd-803cffd5066b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 过滤频次\n",
    "FREQ_LIMIT = 1\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > FREQ_LIMIT]for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bad7db8-5739-4005-83a8-7d44ca6dd8fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构建词典和语料库\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1790c9-5497-44ef-a437-9d309c2f1bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics: 2, Perplexity: -4.523738746531308\n",
      "Number of Topics: 3, Perplexity: -4.55726279062219\n",
      "Number of Topics: 4, Perplexity: -4.488229854032397\n",
      "Number of Topics: 5, Perplexity: -4.525263579795137\n",
      "Number of Topics: 6, Perplexity: -4.49866301426664\n",
      "Number of Topics: 7, Perplexity: -4.5526154530234635\n",
      "Number of Topics: 8, Perplexity: -4.5836647662799805\n",
      "Number of Topics: 9, Perplexity: -4.666694604791701\n",
      "Number of Topics: 10, Perplexity: -4.661523489980027\n",
      "Number of Topics: 11, Perplexity: -4.604664154583588\n",
      "Number of Topics: 12, Perplexity: -4.731867375550792\n",
      "Number of Topics: 13, Perplexity: -4.762198238633573\n",
      "Number of Topics: 14, Perplexity: -4.770978930406272\n",
      "Number of Topics: 15, Perplexity: -4.691590656293556\n",
      "Number of Topics: 16, Perplexity: -4.841862112982199\n",
      "Number of Topics: 17, Perplexity: -4.863777047488838\n",
      "Number of Topics: 18, Perplexity: -4.809053752804175\n",
      "Number of Topics: 19, Perplexity: -4.811820663278922\n",
      "Number of Topics: 20, Perplexity: -4.795252577401698\n",
      "Best number of topics: 17\n"
     ]
    }
   ],
   "source": [
    "# 训练多个LDA模型，并计算每个模型的困惑度\n",
    "models_perplexity = []\n",
    "num_topics_range = range(2, 21)  # 从2到20个主题\n",
    "\n",
    "for num_topics in num_topics_range:\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    models_perplexity.append((num_topics, perplexity))\n",
    "\n",
    "# 打印每个模型的主题数和困惑度\n",
    "for num_topics, perplexity in models_perplexity:\n",
    "    print(f\"Number of Topics: {num_topics}, Perplexity: {perplexity}\")\n",
    "\n",
    "# 选择困惑度最低的模型\n",
    "best_num_topics = min(models_perplexity, key=lambda x: x[1])[0]\n",
    "print(f\"Best number of topics: {best_num_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d81c9a3-aa62-4cf0-ba7e-f84e69eae92d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d8081be910>,\n",
       " <matplotlib.lines.Line2D at 0x1d8081be970>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(models_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb663406-0b36-471f-b1fa-f1618d4de334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_topic = 5\n",
    "lda = models.LdaModel(corpus, num_topics=num_topic, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f89bf8-1d84-4f4f-a26c-dc6ad5b8f7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.5530402815315841\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# 计算主题一致性\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence Score: ', coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d307cd62-1798-4f22-a593-8ffe4803e0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.886810756384701"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='u_mass').get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3387b1ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:37:19.467595Z",
     "start_time": "2024-05-03T06:37:19.445593Z"
    }
   },
   "outputs": [],
   "source": [
    "def topic_data_fill(topic_distribution, topic_num):\n",
    "    features = []\n",
    "    if len(topic_distribution) < topic_num:\n",
    "        features = np.zeros(topic_num).tolist()\n",
    "        for topic in topic_distribution:\n",
    "            features[topic[0]] = topic[1]\n",
    "    else:\n",
    "        features = [topic[1] for topic in topic_distribution]\n",
    "    return features\n",
    "\n",
    "# 提取主题向量\n",
    "lda_features = []\n",
    "for document in corpus:\n",
    "    topic_distribution = lda.get_document_topics(document)\n",
    "    lda_features.append(topic_data_fill(topic_distribution, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99fb7c13-92c5-4cf0-858d-85f53cd163c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.072*\"万\" + 0.072*\"穷人\" + 0.060*\"贫富差距\" + 0.048*\"增加\"')\n",
      "(1, '0.048*\"经济\" + 0.048*\"散户\" + 0.048*\"高位\" + 0.048*\"接盘\"')\n",
      "(2, '0.077*\"贪婪\" + 0.058*\"调整\" + 0.058*\"恐惧\" + 0.049*\"点\"')\n",
      "(3, '0.109*\"doge\" + 0.084*\"都\" + 0.046*\"赚钱\" + 0.046*\"涨\"')\n",
      "(4, '0.076*\"都\" + 0.061*\"不\" + 0.061*\"人\" + 0.047*\"没\"')\n"
     ]
    }
   ],
   "source": [
    "topics = lda.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bbd83399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:38:02.928187Z",
     "start_time": "2024-05-03T06:38:02.912987Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 检测程序\n",
    "for idx, features in enumerate(lda_features):\n",
    "    if len(features) < 5:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2af06009-73e3-4951-a818-db001f673d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0666713, 0.066675246, 0.7332992, 0.06668146, 0.06667282],\n",
       " [0.014358741, 0.9422135, 0.014374056, 0.014582295, 0.014471464],\n",
       " [0.9675566, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.01336838, 0.013522732, 0.013396139, 0.0136795435, 0.9460332],\n",
       " [0.022514543, 0.022406977, 0.91019, 0.022232354, 0.022656156],\n",
       " [0.018209958, 0.018394316, 0.018244436, 0.018232977, 0.92691827],\n",
       " [0.03339818, 0.033581927, 0.033783875, 0.03334335, 0.86589265],\n",
       " [0.050142616, 0.050672363, 0.05012305, 0.7990039, 0.050058074],\n",
       " [0.025153475, 0.89887404, 0.025136646, 0.025176534, 0.025659302],\n",
       " [0.0223808, 0.022420384, 0.022389313, 0.022754857, 0.9100546],\n",
       " [0.983284, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.020148303, 0.020268064, 0.020066217, 0.020046353, 0.919471],\n",
       " [0.066968866, 0.066674516, 0.066910475, 0.7319446, 0.06750155],\n",
       " [0.050004303, 0.796373, 0.05020461, 0.05209535, 0.05132273],\n",
       " [0.02521658, 0.025109377, 0.025180638, 0.025073107, 0.8994203],\n",
       " [0.033459716, 0.03369006, 0.03342471, 0.86537135, 0.034054145],\n",
       " [0.068290435, 0.72967273, 0.067099124, 0.066683926, 0.06825383],\n",
       " [0.033335187, 0.86653334, 0.033334855, 0.03333922, 0.033457365],\n",
       " [0.040003087, 0.040258195, 0.6514643, 0.22810602, 0.04016841],\n",
       " [0.0, 0.0, 0.9897927, 0.0, 0.0]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ab622c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:40:43.022516Z",
     "start_time": "2024-05-03T06:40:43.009517Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "D:\\tool\\miniconda3\\envs\\allInOne\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertModel were not initialized from the model checkpoint at D:\\tool\\toolkit\\nlp\\distiluse-base-multilingual-cased-v2-finetuned-stsb_multi_mt-es and are newly initialized: ['embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 加载已经训练好的bert模型\n",
    "model_path = r'D:\\tool\\toolkit\\nlp\\distiluse-base-multilingual-cased-v2-finetuned-stsb_multi_mt-es'\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b56c95-9b2a-43bd-8848-18b29e0bb28f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_features = []\n",
    "for article in doc_data:\n",
    "    # 对文章进行分词和编码\n",
    "    encoded_input = tokenizer(article, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    # 获取模型的输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    # 获取CLS token的嵌入作为文章的特征向量\n",
    "    bert_features.append(outputs.last_hidden_state[:, 0, :].squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "962648a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:39:13.503336Z",
     "start_time": "2024-05-03T06:39:13.488177Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lda和bert特征拼接\n",
    "concatenated_features = []\n",
    "for bert_feature, lda_feature in zip(bert_features, lda_features):\n",
    "    # 将BERT特征向量和LDA主题向量拼接\n",
    "    concatenated_feature = np.concatenate((bert_feature, lda_feature))\n",
    "    concatenated_features.append(concatenated_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ad01ab-7567-45c7-b4a7-d2f21b51bafd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义自编码器\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder  = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.en_fc = nn.Linear(64, hidden_size)\n",
    "        self.de_fc = nn.Linear(hidden_size, 64)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        en = self.encoder(x)\n",
    "        code = self.en_fc(en)\n",
    "        de = self.de_fc(code)\n",
    "        decoded = self.decoder(de)\n",
    "        return code, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ef2eb34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:02:58.651656Z",
     "start_time": "2024-05-03T07:02:58.429432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [1/1], Loss: 0.9966\n",
      "Epoch [2/5], Batch [1/1], Loss: 0.9836\n",
      "Epoch [3/5], Batch [1/1], Loss: 0.9666\n",
      "Epoch [4/5], Batch [1/1], Loss: 0.9431\n",
      "Epoch [5/5], Batch [1/1], Loss: 0.9095\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "# 模型默认参数为float32，如果想要用double(float64)来训练的话，model=model.double()\n",
    "tensor_data = torch.tensor(concatenated_features, dtype=torch.float32)\n",
    "dataset = TensorDataset(tensor_data)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 开始训练\n",
    "HIDDEN_SIZE = 32\n",
    "input_dim = len(concatenated_features[0])\n",
    "\n",
    "model = AutoEncoder(input_dim, HIDDEN_SIZE)\n",
    "criterion = nn.MSELoss()  # 均方误差损失\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "model = model.to(\"cpu\")\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data,) in enumerate(data_loader):\n",
    "        # 正向传播\n",
    "        outputs = model(data)[1]\n",
    "        loss = criterion(outputs, data)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecf3a300-cf9d-4712-a8d5-7a5c68627298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取文本encoder后的结果\n",
    "encoder_features = model(dataset)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67de8034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:04:32.191987Z",
     "start_time": "2024-05-03T07:04:32.176893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 773])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对encoder_features进行聚类分析\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 假设 encoder_features 是一个二维数组，其中每一行是一个文档的特征向量\n",
    "# texts 是原始文本数据列表\n",
    "\n",
    "# 使用 K-means 算法进行聚类，设置聚类数目为 5\n",
    "kmeans = KMeans(n_clusters=5, random_state=42).fit(encoder_features)\n",
    "\n",
    "# 获取每个文档的聚类标签\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 假设 vectorizer 是之前用于将文本转换为特征向量的 TF-IDF 向量化器\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 获取特征名称（词汇）\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 为每个聚类提取主题词\n",
    "def get_cluster_topic(cluster_label):\n",
    "    # 选择当前聚类的所有文档\n",
    "    cluster_docs = [doc for doc_idx, label in enumerate(cluster_labels) if label == cluster_label]\n",
    "    \n",
    "    # 将文档转换为词袋模型\n",
    "    cluster_bow = [vectorizer.transform([doc]) for doc in cluster_docs]\n",
    "    \n",
    "    # 计算每个词在当前聚类中的总TF-IDF值\n",
    "    cluster_word_tfidf_sum = sum(cluster_bow)\n",
    "    \n",
    "    # 获取每个词的总TF-IDF值，并按降序排列\n",
    "    word_tfidf_scores = cluster_word_tfidf_sum.toarray().flatten().tolist()\n",
    "    word_tfidf_scores = sorted(enumerate(word_tfidf_scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 返回TF-IDF值最高的N个词作为主题词\n",
    "    top_n_words = set([feature_names[word_idx] for word_idx, _ in word_tfidf_scores[:10]])\n",
    "    return top_n_words\n",
    "\n",
    "# 为每个聚类提取主题词\n",
    "cluster_topics = {label: get_cluster_topic(label) for label in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4550dda9-c0c5-4f77-984a-71cd156c4186",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 使用TF-IDF向量化文本\u001b[39;00m\n\u001b[0;32m      8\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 应用K-means聚类\u001b[39;00m\n\u001b[0;32m     12\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X)\n",
      "File \u001b[1;32mD:\\tool\\miniconda3\\envs\\allInOne\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mD:\\tool\\miniconda3\\envs\\allInOne\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\tool\\miniconda3\\envs\\allInOne\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1393\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1391\u001b[0m min_doc_count \u001b[38;5;241m=\u001b[39m min_df \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(min_df, Integral) \u001b[38;5;28;01melse\u001b[39;00m min_df \u001b[38;5;241m*\u001b[39m n_doc\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_doc_count \u001b[38;5;241m<\u001b[39m min_doc_count:\n\u001b[1;32m-> 1393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_df corresponds to < documents than min_df\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1395\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
      "\u001b[1;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 假设documents是原始文本数据列表\n",
    "documents = [\"text of document 1\", \"text of document 2\"]\n",
    "\n",
    "# 使用TF-IDF向量化文本\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 应用K-means聚类\n",
    "kmeans = KMeans(n_clusters=5).fit(X)\n",
    "\n",
    "# 为每个簇提取主题词\n",
    "def get_cluster_words(cluster_documents):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    cluster_words = []\n",
    "    for doc in cluster_documents:\n",
    "        for word in doc.split():\n",
    "            if word in words:\n",
    "                cluster_words.append(word)\n",
    "    # 计算词频并选择最常见的词\n",
    "    freq_dist = nltk.FreqDist(cluster_words)\n",
    "    common_words = freq_dist.most_common(10)\n",
    "    return [word for word, freq in common_words]\n",
    "\n",
    "# 为每个簇提取主题词\n",
    "cluster_topics = {i: get_cluster_words(documents[k]) for i, k in enumerate(kmeans.labels_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ace5782f-46d1-4332-bb2a-9c74e3eb2fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 示例文本数据\n",
    "documents = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# 初始化TF-IDF向量化器\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "\n",
    "# 将文本数据转换为TF-IDF特征矩阵\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 获取特征名称（词汇）\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 查看每个文档的TF-IDF向量\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a7342756-bb04-4b3a-bb61-36f891a2ab7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['document']\n",
      "TF-IDF Matrix:\n",
      " [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# 查看每个文档的TF-IDF向量\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"TF-IDF Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb7c25-5b8e-48ba-9f04-071c99946ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r'C:\\Users\\11435\\Desktop\\clutter\\research\\data\\others\\互动\\互动\\用户与公司问答-2010.xlsx'\n",
    "df = pd.read_excel(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38257f99-9c72-4e5f-acff-dbd7156f7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载已经训练好的bert模型\n",
    "model_path = r'D:\\tool\\toolkit\\nlp\\distiluse-base-multilingual-cased-v2-finetuned-stsb_multi_mt-es'\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496d91e-931c-4ad1-9419-3ab8158b2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_features = []\n",
    "for article in doc_data:\n",
    "    # 对文章进行分词和编码\n",
    "    encoded_input = tokenizer(article, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    # 获取模型的输出\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    # 获取CLS token的嵌入作为文章的特征向量\n",
    "    bert_features.append(outputs.last_hidden_state[:, 0, :].squeeze().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
